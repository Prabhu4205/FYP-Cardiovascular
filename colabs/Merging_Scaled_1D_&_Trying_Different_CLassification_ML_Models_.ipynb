{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rameshavinash94/Cardiovascular-Detection-using-ECG-images/blob/main/Merging_Scaled_1D_%26_Trying_Different_CLassification_ML_Models_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIqvxE_BKjV6"
   },
   "source": [
    "### IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C82bu1OsKnfC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from natsort import natsorted\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXx7SoC7qyRv"
   },
   "source": [
    "### **WORKING ON COMBING MULTIPLE LEAD FILES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sI0sFzAqPP8g"
   },
   "outputs": [],
   "source": [
    "#creating list to store file_names\n",
    "NORMAL_=[]\n",
    "MI_=[]\n",
    "PMI_=[]\n",
    "HB_=[]\n",
    "\n",
    "normal = r\"D:\\Cardiovascular-Detection-using-ECG-images\\preprocessed_1d\\NORMAL\"\n",
    "abnormal = r\"D:\\Cardiovascular-Detection-using-ECG-images\\preprocessed_1d\\AHB\"\n",
    "MI = r\"D:\\Cardiovascular-Detection-using-ECG-images\\preprocessed_1d\\MI\"\n",
    "MI_history = r\"D:\\Cardiovascular-Detection-using-ECG-images\\preprocessed_1d\\PM\"\n",
    "\n",
    "Types_ECG = {'normal':normal,'Abnormal_hear_beat':abnormal,'MI':MI,'History_MI':MI_history}\n",
    "\n",
    "for types,folder in Types_ECG.items():\n",
    "  for files in os.listdir(folder):\n",
    "    if types=='normal':\n",
    "      NORMAL_.append(files)\n",
    "    elif types=='Abnormal_hear_beat':\n",
    "      HB_.append(files)\n",
    "    elif types=='MI':\n",
    "      MI_.append(files)\n",
    "    elif types=='History_MI':\n",
    "      PMI_.append(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AJL9qAFSUOsN",
    "outputId": "9a4817f5-ae50-4aaf-96fe-8cc945289ad7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaled_data_1D_1.csv',\n",
       " 'scaled_data_1D_2.csv',\n",
       " 'scaled_data_1D_3.csv',\n",
       " 'scaled_data_1D_4.csv',\n",
       " 'scaled_data_1D_5.csv',\n",
       " 'scaled_data_1D_6.csv',\n",
       " 'scaled_data_1D_7.csv',\n",
       " 'scaled_data_1D_8.csv',\n",
       " 'scaled_data_1D_9.csv',\n",
       " 'scaled_data_1D_10.csv',\n",
       " 'scaled_data_1D_11.csv',\n",
       " 'scaled_data_1D_12.csv',\n",
       " 'scaled_data_1D_13.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NORMAL_ = natsorted(NORMAL_)\n",
    "NORMAL_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BRbNR2HEUkvU",
    "outputId": "0cd461c0-5fe2-463a-cbf5-6325662d9011"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaled_data_1D_1.csv',\n",
       " 'scaled_data_1D_2.csv',\n",
       " 'scaled_data_1D_3.csv',\n",
       " 'scaled_data_1D_4.csv',\n",
       " 'scaled_data_1D_5.csv',\n",
       " 'scaled_data_1D_6.csv',\n",
       " 'scaled_data_1D_7.csv',\n",
       " 'scaled_data_1D_8.csv',\n",
       " 'scaled_data_1D_9.csv',\n",
       " 'scaled_data_1D_10.csv',\n",
       " 'scaled_data_1D_11.csv',\n",
       " 'scaled_data_1D_12.csv',\n",
       " 'scaled_data_1D_13.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MI_ = natsorted(MI_)\n",
    "MI_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4LMfTU3UuCM",
    "outputId": "0c127e8e-62b7-4015-e3ee-a34860e4f5ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaled_data_1D_1.csv',\n",
       " 'scaled_data_1D_2.csv',\n",
       " 'scaled_data_1D_3.csv',\n",
       " 'scaled_data_1D_4.csv',\n",
       " 'scaled_data_1D_5.csv',\n",
       " 'scaled_data_1D_6.csv',\n",
       " 'scaled_data_1D_7.csv',\n",
       " 'scaled_data_1D_8.csv',\n",
       " 'scaled_data_1D_9.csv',\n",
       " 'scaled_data_1D_10.csv',\n",
       " 'scaled_data_1D_11.csv',\n",
       " 'scaled_data_1D_12.csv',\n",
       " 'scaled_data_1D_13.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PMI_ = natsorted(PMI_)\n",
    "PMI_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OinhdMKtU5mf",
    "outputId": "52dd393b-76c2-44a9-e64e-4d428380ed8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaled_data_1D_1.csv',\n",
       " 'scaled_data_1D_2.csv',\n",
       " 'scaled_data_1D_3.csv',\n",
       " 'scaled_data_1D_4.csv',\n",
       " 'scaled_data_1D_5.csv',\n",
       " 'scaled_data_1D_6.csv',\n",
       " 'scaled_data_1D_7.csv',\n",
       " 'scaled_data_1D_8.csv',\n",
       " 'scaled_data_1D_9.csv',\n",
       " 'scaled_data_1D_10.csv',\n",
       " 'scaled_data_1D_11.csv',\n",
       " 'scaled_data_1D_12.csv',\n",
       " 'scaled_data_1D_13.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HB_ = natsorted(HB_)\n",
    "HB_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5T4Ay50ch6Ru"
   },
   "source": [
    "#### **COMBINED CSV OF EACH LEAD(1-12) FROM ALL IMAGES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Ot65BTX2VF7Y"
   },
   "outputs": [],
   "source": [
    "#loop over and create combined csv files for each leads.\n",
    "for x in range(len(MI_)):\n",
    "  df1=pd.read_csv(r\"D:\\Cardiovascular-Detection-using-ECG-images\\preprocessed_1d\\NORMAL\\{}\".format(NORMAL_[x]))\n",
    "  df2=pd.read_csv(r\"D:\\Cardiovascular-Detection-using-ECG-images\\preprocessed_1d\\AHB\\{}\".format(HB_[x]))\n",
    "  df3=pd.read_csv(r\"D:\\Cardiovascular-Detection-using-ECG-images\\preprocessed_1d\\MI\\{}\".format(MI_[x]))\n",
    "  df4=pd.read_csv(r\"D:\\Cardiovascular-Detection-using-ECG-images\\preprocessed_1d\\PM\\{}\".format(PMI_[x]))\n",
    "  final_df = pd.concat([df1,df2,df3,df4],ignore_index=True)\n",
    "  final_df.to_csv(r'D:\\Cardiovascular-Detection-using-ECG-images\\preprocessed_1d\\Combined_IDLead_{}.csv'.format(x+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RyTYwqJSX8Vh",
    "outputId": "b1c922e9-83f4-4d31-e5cd-d5b18a78acf0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No', 'HB', 'MI', 'PM'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now reading just lead1\n",
    "df=pd.read_csv(r'D:\\Cardiovascular-Detection-using-ECG-images\\preprocessed_1d\\Combined_IDLead_1.csv')\n",
    "df['Target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nbSKpUc2angF"
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "fX_uYtekhL_z",
    "outputId": "a6d14a8e-a75f-4b1b-f39f-7830cba4c581"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.728449</td>\n",
       "      <td>0.680755</td>\n",
       "      <td>0.619010</td>\n",
       "      <td>0.645367</td>\n",
       "      <td>0.681570</td>\n",
       "      <td>0.732488</td>\n",
       "      <td>0.758448</td>\n",
       "      <td>0.750660</td>\n",
       "      <td>0.728282</td>\n",
       "      <td>0.707928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.637260</td>\n",
       "      <td>0.664539</td>\n",
       "      <td>0.667226</td>\n",
       "      <td>0.637064</td>\n",
       "      <td>0.593287</td>\n",
       "      <td>0.545503</td>\n",
       "      <td>0.515049</td>\n",
       "      <td>0.563257</td>\n",
       "      <td>0.633581</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.957972</td>\n",
       "      <td>0.950695</td>\n",
       "      <td>0.941024</td>\n",
       "      <td>0.930501</td>\n",
       "      <td>0.913601</td>\n",
       "      <td>0.892244</td>\n",
       "      <td>0.868016</td>\n",
       "      <td>0.855127</td>\n",
       "      <td>0.835307</td>\n",
       "      <td>0.798640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778790</td>\n",
       "      <td>0.806883</td>\n",
       "      <td>0.818640</td>\n",
       "      <td>0.842472</td>\n",
       "      <td>0.866740</td>\n",
       "      <td>0.884152</td>\n",
       "      <td>0.897196</td>\n",
       "      <td>0.911293</td>\n",
       "      <td>0.922903</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.611084</td>\n",
       "      <td>0.661575</td>\n",
       "      <td>0.695790</td>\n",
       "      <td>0.741113</td>\n",
       "      <td>0.716666</td>\n",
       "      <td>0.595794</td>\n",
       "      <td>0.425022</td>\n",
       "      <td>0.286457</td>\n",
       "      <td>0.425022</td>\n",
       "      <td>0.611384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042690</td>\n",
       "      <td>0.165850</td>\n",
       "      <td>0.363445</td>\n",
       "      <td>0.549460</td>\n",
       "      <td>0.539346</td>\n",
       "      <td>0.522272</td>\n",
       "      <td>0.491668</td>\n",
       "      <td>0.454949</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.839213</td>\n",
       "      <td>0.861690</td>\n",
       "      <td>0.866457</td>\n",
       "      <td>0.865756</td>\n",
       "      <td>0.855027</td>\n",
       "      <td>0.855606</td>\n",
       "      <td>0.845561</td>\n",
       "      <td>0.843187</td>\n",
       "      <td>0.846784</td>\n",
       "      <td>0.824438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.789156</td>\n",
       "      <td>0.793622</td>\n",
       "      <td>0.787665</td>\n",
       "      <td>0.794515</td>\n",
       "      <td>0.796739</td>\n",
       "      <td>0.804063</td>\n",
       "      <td>0.809944</td>\n",
       "      <td>0.801814</td>\n",
       "      <td>0.777322</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.917753</td>\n",
       "      <td>0.924369</td>\n",
       "      <td>0.873765</td>\n",
       "      <td>0.791381</td>\n",
       "      <td>0.699513</td>\n",
       "      <td>0.604927</td>\n",
       "      <td>0.500312</td>\n",
       "      <td>0.446012</td>\n",
       "      <td>0.528910</td>\n",
       "      <td>0.634068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200676</td>\n",
       "      <td>0.300147</td>\n",
       "      <td>0.407225</td>\n",
       "      <td>0.507346</td>\n",
       "      <td>0.605953</td>\n",
       "      <td>0.699309</td>\n",
       "      <td>0.790334</td>\n",
       "      <td>0.856593</td>\n",
       "      <td>0.849957</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>0.874246</td>\n",
       "      <td>0.877014</td>\n",
       "      <td>0.864280</td>\n",
       "      <td>0.860505</td>\n",
       "      <td>0.871349</td>\n",
       "      <td>0.912404</td>\n",
       "      <td>0.958148</td>\n",
       "      <td>0.977826</td>\n",
       "      <td>0.956314</td>\n",
       "      <td>0.926773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.908312</td>\n",
       "      <td>0.926328</td>\n",
       "      <td>0.898749</td>\n",
       "      <td>0.855709</td>\n",
       "      <td>0.823132</td>\n",
       "      <td>0.815458</td>\n",
       "      <td>0.818083</td>\n",
       "      <td>0.829300</td>\n",
       "      <td>0.822382</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>0.829815</td>\n",
       "      <td>0.832084</td>\n",
       "      <td>0.852396</td>\n",
       "      <td>0.909665</td>\n",
       "      <td>0.988242</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923323</td>\n",
       "      <td>0.821865</td>\n",
       "      <td>0.721302</td>\n",
       "      <td>0.612039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429721</td>\n",
       "      <td>0.531567</td>\n",
       "      <td>0.642137</td>\n",
       "      <td>0.742063</td>\n",
       "      <td>0.833042</td>\n",
       "      <td>0.814867</td>\n",
       "      <td>0.777622</td>\n",
       "      <td>0.760714</td>\n",
       "      <td>0.759294</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>0.469048</td>\n",
       "      <td>0.417983</td>\n",
       "      <td>0.362322</td>\n",
       "      <td>0.351995</td>\n",
       "      <td>0.391493</td>\n",
       "      <td>0.418305</td>\n",
       "      <td>0.440135</td>\n",
       "      <td>0.444598</td>\n",
       "      <td>0.460402</td>\n",
       "      <td>0.506810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408587</td>\n",
       "      <td>0.401864</td>\n",
       "      <td>0.387069</td>\n",
       "      <td>0.359590</td>\n",
       "      <td>0.325879</td>\n",
       "      <td>0.288894</td>\n",
       "      <td>0.293521</td>\n",
       "      <td>0.344504</td>\n",
       "      <td>0.399012</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>0.682510</td>\n",
       "      <td>0.682286</td>\n",
       "      <td>0.641051</td>\n",
       "      <td>0.620212</td>\n",
       "      <td>0.608210</td>\n",
       "      <td>0.576331</td>\n",
       "      <td>0.603596</td>\n",
       "      <td>0.645714</td>\n",
       "      <td>0.677964</td>\n",
       "      <td>0.720297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.452247</td>\n",
       "      <td>0.450421</td>\n",
       "      <td>0.439278</td>\n",
       "      <td>0.439086</td>\n",
       "      <td>0.394417</td>\n",
       "      <td>0.441650</td>\n",
       "      <td>0.473909</td>\n",
       "      <td>0.539199</td>\n",
       "      <td>0.547146</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>0.792175</td>\n",
       "      <td>0.815695</td>\n",
       "      <td>0.819518</td>\n",
       "      <td>0.820559</td>\n",
       "      <td>0.847985</td>\n",
       "      <td>0.880933</td>\n",
       "      <td>0.902061</td>\n",
       "      <td>0.878266</td>\n",
       "      <td>0.838806</td>\n",
       "      <td>0.811795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.737351</td>\n",
       "      <td>0.778845</td>\n",
       "      <td>0.805446</td>\n",
       "      <td>0.782640</td>\n",
       "      <td>0.751236</td>\n",
       "      <td>0.741331</td>\n",
       "      <td>0.718790</td>\n",
       "      <td>0.714504</td>\n",
       "      <td>0.691004</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.728449  0.680755  0.619010  0.645367  0.681570  0.732488  0.758448   \n",
       "1    0.957972  0.950695  0.941024  0.930501  0.913601  0.892244  0.868016   \n",
       "2    0.611084  0.661575  0.695790  0.741113  0.716666  0.595794  0.425022   \n",
       "3    0.839213  0.861690  0.866457  0.865756  0.855027  0.855606  0.845561   \n",
       "4    0.917753  0.924369  0.873765  0.791381  0.699513  0.604927  0.500312   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "923  0.874246  0.877014  0.864280  0.860505  0.871349  0.912404  0.958148   \n",
       "924  0.829815  0.832084  0.852396  0.909665  0.988242  1.000000  0.923323   \n",
       "925  0.469048  0.417983  0.362322  0.351995  0.391493  0.418305  0.440135   \n",
       "926  0.682510  0.682286  0.641051  0.620212  0.608210  0.576331  0.603596   \n",
       "927  0.792175  0.815695  0.819518  0.820559  0.847985  0.880933  0.902061   \n",
       "\n",
       "            7         8         9  ...       246       247       248  \\\n",
       "0    0.750660  0.728282  0.707928  ...  0.637260  0.664539  0.667226   \n",
       "1    0.855127  0.835307  0.798640  ...  0.778790  0.806883  0.818640   \n",
       "2    0.286457  0.425022  0.611384  ...  0.000000  0.042690  0.165850   \n",
       "3    0.843187  0.846784  0.824438  ...  0.789156  0.793622  0.787665   \n",
       "4    0.446012  0.528910  0.634068  ...  0.200676  0.300147  0.407225   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "923  0.977826  0.956314  0.926773  ...  0.908312  0.926328  0.898749   \n",
       "924  0.821865  0.721302  0.612039  ...  0.429721  0.531567  0.642137   \n",
       "925  0.444598  0.460402  0.506810  ...  0.408587  0.401864  0.387069   \n",
       "926  0.645714  0.677964  0.720297  ...  0.452247  0.450421  0.439278   \n",
       "927  0.878266  0.838806  0.811795  ...  0.737351  0.778845  0.805446   \n",
       "\n",
       "          249       250       251       252       253       254  target  \n",
       "0    0.637064  0.593287  0.545503  0.515049  0.563257  0.633581       2  \n",
       "1    0.842472  0.866740  0.884152  0.897196  0.911293  0.922903       2  \n",
       "2    0.363445  0.549460  0.539346  0.522272  0.491668  0.454949       2  \n",
       "3    0.794515  0.796739  0.804063  0.809944  0.801814  0.777322       2  \n",
       "4    0.507346  0.605953  0.699309  0.790334  0.856593  0.849957       2  \n",
       "..        ...       ...       ...       ...       ...       ...     ...  \n",
       "923  0.855709  0.823132  0.815458  0.818083  0.829300  0.822382       3  \n",
       "924  0.742063  0.833042  0.814867  0.777622  0.760714  0.759294       3  \n",
       "925  0.359590  0.325879  0.288894  0.293521  0.344504  0.399012       3  \n",
       "926  0.439086  0.394417  0.441650  0.473909  0.539199  0.547146       3  \n",
       "927  0.782640  0.751236  0.741331  0.718790  0.714504  0.691004       3  \n",
       "\n",
       "[928 rows x 256 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert Target column values as Numeric using ngroups\n",
    "encode_target_label = df.groupby('Target').ngroup().rename(\"target\").to_frame()\n",
    "test_final  = df.merge(encode_target_label, left_index=True, right_index=True)\n",
    "test_final.drop(columns=['Target'],inplace=True)\n",
    "test_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRvRDvfrivtE"
   },
   "source": [
    "#### **PERFORM DIMENSIONALITY REDUCTION JUST FOR CHECKING/UNDERSTANDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 912
    },
    "id": "scVPRQ3TZBaW",
    "outputId": "0b606399-2d4a-40a0-b9e3-cd7592305d67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of each component: [1.76145888e-01 9.50265614e-02 6.99060614e-02 6.15960001e-02\n",
      " 5.34876630e-02 4.23664893e-02 3.68320213e-02 3.38541791e-02\n",
      " 3.00884979e-02 2.90396728e-02 2.64962509e-02 2.42272738e-02\n",
      " 2.10221030e-02 1.99751559e-02 1.77321042e-02 1.63016802e-02\n",
      " 1.53898622e-02 1.48412074e-02 1.33644825e-02 1.19674074e-02\n",
      " 1.16813409e-02 1.05807650e-02 9.68875480e-03 9.47385060e-03\n",
      " 8.65347748e-03 8.47506998e-03 7.93382172e-03 7.30163338e-03\n",
      " 6.76380665e-03 6.36886390e-03 6.02004791e-03 5.46823032e-03\n",
      " 5.31229911e-03 4.97821789e-03 4.74686092e-03 4.46081684e-03\n",
      " 4.21254684e-03 4.01200243e-03 3.87246476e-03 3.52519084e-03\n",
      " 3.37596894e-03 3.26978336e-03 3.08241145e-03 2.96423495e-03\n",
      " 2.73419816e-03 2.50965698e-03 2.35335480e-03 2.25665349e-03\n",
      " 2.20141761e-03 1.96782025e-03 1.74343954e-03 1.70982830e-03\n",
      " 1.57456047e-03 1.53704487e-03 1.36768435e-03 1.33167096e-03\n",
      " 1.26444173e-03 1.20053330e-03 1.18738749e-03 1.08864087e-03\n",
      " 1.02824532e-03 9.11484783e-04 7.89962329e-04 7.59785111e-04\n",
      " 6.49920865e-04 6.27833793e-04 6.04784065e-04 5.45886709e-04\n",
      " 5.32310105e-04 4.97728350e-04 4.78393197e-04 4.50404968e-04\n",
      " 4.26173491e-04 4.09392368e-04 3.92601455e-04 3.70241592e-04\n",
      " 3.66854851e-04 3.38846661e-04 3.08205827e-04 3.00634264e-04\n",
      " 2.80363539e-04 2.67851033e-04 2.49661214e-04 2.40638979e-04\n",
      " 2.11564443e-04 2.03939307e-04 1.99025184e-04 1.83142032e-04\n",
      " 1.66818708e-04 1.65884106e-04 1.55950830e-04 1.39179221e-04\n",
      " 1.25325946e-04 1.22326428e-04 1.13656986e-04 1.09459993e-04\n",
      " 1.03362277e-04 9.93653641e-05 9.86126977e-05 9.42379392e-05]\n",
      "\n",
      " Total Variance Explained: 99.8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.018578</td>\n",
       "      <td>1.148263</td>\n",
       "      <td>-0.589582</td>\n",
       "      <td>0.193617</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>-0.309400</td>\n",
       "      <td>-0.161566</td>\n",
       "      <td>0.478471</td>\n",
       "      <td>0.972403</td>\n",
       "      <td>-0.031832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012254</td>\n",
       "      <td>-0.003815</td>\n",
       "      <td>0.025192</td>\n",
       "      <td>0.010857</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>0.003283</td>\n",
       "      <td>0.007735</td>\n",
       "      <td>-0.065010</td>\n",
       "      <td>-0.028622</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.098692</td>\n",
       "      <td>0.289832</td>\n",
       "      <td>-1.766388</td>\n",
       "      <td>1.076165</td>\n",
       "      <td>-0.261201</td>\n",
       "      <td>-0.820446</td>\n",
       "      <td>-0.474188</td>\n",
       "      <td>-0.515238</td>\n",
       "      <td>0.692389</td>\n",
       "      <td>1.501606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017533</td>\n",
       "      <td>0.060498</td>\n",
       "      <td>-0.007995</td>\n",
       "      <td>0.013084</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.045964</td>\n",
       "      <td>0.032976</td>\n",
       "      <td>-0.035506</td>\n",
       "      <td>0.002295</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.275021</td>\n",
       "      <td>-0.451289</td>\n",
       "      <td>0.106750</td>\n",
       "      <td>-0.426415</td>\n",
       "      <td>0.066133</td>\n",
       "      <td>0.692474</td>\n",
       "      <td>0.634894</td>\n",
       "      <td>-0.035867</td>\n",
       "      <td>0.815855</td>\n",
       "      <td>-0.909473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045105</td>\n",
       "      <td>-0.042407</td>\n",
       "      <td>-0.028622</td>\n",
       "      <td>0.120064</td>\n",
       "      <td>-0.046621</td>\n",
       "      <td>-0.022233</td>\n",
       "      <td>-0.020058</td>\n",
       "      <td>0.048230</td>\n",
       "      <td>0.007981</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.517085</td>\n",
       "      <td>1.662693</td>\n",
       "      <td>-1.021167</td>\n",
       "      <td>0.804267</td>\n",
       "      <td>-0.281985</td>\n",
       "      <td>0.518180</td>\n",
       "      <td>0.355748</td>\n",
       "      <td>-0.344235</td>\n",
       "      <td>-0.910867</td>\n",
       "      <td>-0.629517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.042739</td>\n",
       "      <td>0.015855</td>\n",
       "      <td>-0.050769</td>\n",
       "      <td>-0.038103</td>\n",
       "      <td>-0.014820</td>\n",
       "      <td>-0.022481</td>\n",
       "      <td>0.027135</td>\n",
       "      <td>-0.036524</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.152840</td>\n",
       "      <td>-1.046283</td>\n",
       "      <td>0.351278</td>\n",
       "      <td>1.100381</td>\n",
       "      <td>-1.613642</td>\n",
       "      <td>1.484188</td>\n",
       "      <td>-0.113277</td>\n",
       "      <td>-0.251152</td>\n",
       "      <td>0.179023</td>\n",
       "      <td>-0.233104</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008807</td>\n",
       "      <td>0.054492</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.029601</td>\n",
       "      <td>0.016971</td>\n",
       "      <td>-0.001190</td>\n",
       "      <td>0.047220</td>\n",
       "      <td>-0.000497</td>\n",
       "      <td>-0.024342</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>-1.321884</td>\n",
       "      <td>2.153021</td>\n",
       "      <td>0.788596</td>\n",
       "      <td>-1.304253</td>\n",
       "      <td>0.458186</td>\n",
       "      <td>-0.859346</td>\n",
       "      <td>-0.069127</td>\n",
       "      <td>-0.392796</td>\n",
       "      <td>0.755732</td>\n",
       "      <td>-0.584050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037369</td>\n",
       "      <td>0.077176</td>\n",
       "      <td>-0.044731</td>\n",
       "      <td>-0.031645</td>\n",
       "      <td>0.009314</td>\n",
       "      <td>-0.022128</td>\n",
       "      <td>-0.049114</td>\n",
       "      <td>-0.007692</td>\n",
       "      <td>0.014365</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>-0.867163</td>\n",
       "      <td>-0.040504</td>\n",
       "      <td>0.940680</td>\n",
       "      <td>0.302648</td>\n",
       "      <td>-0.469672</td>\n",
       "      <td>-0.368255</td>\n",
       "      <td>1.065579</td>\n",
       "      <td>0.801522</td>\n",
       "      <td>0.690113</td>\n",
       "      <td>0.953008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012492</td>\n",
       "      <td>-0.093375</td>\n",
       "      <td>0.014933</td>\n",
       "      <td>-0.028358</td>\n",
       "      <td>-0.009963</td>\n",
       "      <td>0.057517</td>\n",
       "      <td>-0.007851</td>\n",
       "      <td>-0.063690</td>\n",
       "      <td>-0.020377</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>3.753012</td>\n",
       "      <td>0.841636</td>\n",
       "      <td>-0.317393</td>\n",
       "      <td>-0.296117</td>\n",
       "      <td>0.593769</td>\n",
       "      <td>-0.255474</td>\n",
       "      <td>-0.057091</td>\n",
       "      <td>-0.072048</td>\n",
       "      <td>0.664386</td>\n",
       "      <td>-0.837668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008571</td>\n",
       "      <td>-0.006161</td>\n",
       "      <td>-0.003945</td>\n",
       "      <td>0.005146</td>\n",
       "      <td>-0.022885</td>\n",
       "      <td>0.069912</td>\n",
       "      <td>0.021681</td>\n",
       "      <td>-0.007789</td>\n",
       "      <td>-0.010856</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>0.603083</td>\n",
       "      <td>0.126259</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>0.283612</td>\n",
       "      <td>0.169559</td>\n",
       "      <td>-0.156326</td>\n",
       "      <td>-0.068399</td>\n",
       "      <td>-0.184308</td>\n",
       "      <td>0.461063</td>\n",
       "      <td>-0.002047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056376</td>\n",
       "      <td>0.024160</td>\n",
       "      <td>-0.015827</td>\n",
       "      <td>-0.078990</td>\n",
       "      <td>-0.005638</td>\n",
       "      <td>0.035686</td>\n",
       "      <td>-0.056002</td>\n",
       "      <td>0.018786</td>\n",
       "      <td>0.063739</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>-1.452945</td>\n",
       "      <td>1.233599</td>\n",
       "      <td>0.439472</td>\n",
       "      <td>0.278517</td>\n",
       "      <td>0.165928</td>\n",
       "      <td>-0.171830</td>\n",
       "      <td>-0.075000</td>\n",
       "      <td>0.033859</td>\n",
       "      <td>0.004444</td>\n",
       "      <td>-0.687583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012733</td>\n",
       "      <td>-0.030278</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>-0.062381</td>\n",
       "      <td>0.014018</td>\n",
       "      <td>-0.072341</td>\n",
       "      <td>0.007953</td>\n",
       "      <td>-0.053482</td>\n",
       "      <td>-0.029433</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    1.018578  1.148263 -0.589582  0.193617  0.047950 -0.309400 -0.161566   \n",
       "1   -1.098692  0.289832 -1.766388  1.076165 -0.261201 -0.820446 -0.474188   \n",
       "2    0.275021 -0.451289  0.106750 -0.426415  0.066133  0.692474  0.634894   \n",
       "3   -1.517085  1.662693 -1.021167  0.804267 -0.281985  0.518180  0.355748   \n",
       "4   -0.152840 -1.046283  0.351278  1.100381 -1.613642  1.484188 -0.113277   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "923 -1.321884  2.153021  0.788596 -1.304253  0.458186 -0.859346 -0.069127   \n",
       "924 -0.867163 -0.040504  0.940680  0.302648 -0.469672 -0.368255  1.065579   \n",
       "925  3.753012  0.841636 -0.317393 -0.296117  0.593769 -0.255474 -0.057091   \n",
       "926  0.603083  0.126259  0.003433  0.283612  0.169559 -0.156326 -0.068399   \n",
       "927 -1.452945  1.233599  0.439472  0.278517  0.165928 -0.171830 -0.075000   \n",
       "\n",
       "            7         8         9  ...        91        92        93  \\\n",
       "0    0.478471  0.972403 -0.031832  ...  0.012254 -0.003815  0.025192   \n",
       "1   -0.515238  0.692389  1.501606  ... -0.017533  0.060498 -0.007995   \n",
       "2   -0.035867  0.815855 -0.909473  ...  0.045105 -0.042407 -0.028622   \n",
       "3   -0.344235 -0.910867 -0.629517  ...  0.000802  0.042739  0.015855   \n",
       "4   -0.251152  0.179023 -0.233104  ... -0.008807  0.054492  0.001487   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "923 -0.392796  0.755732 -0.584050  ... -0.037369  0.077176 -0.044731   \n",
       "924  0.801522  0.690113  0.953008  ...  0.012492 -0.093375  0.014933   \n",
       "925 -0.072048  0.664386 -0.837668  ... -0.008571 -0.006161 -0.003945   \n",
       "926 -0.184308  0.461063 -0.002047  ...  0.056376  0.024160 -0.015827   \n",
       "927  0.033859  0.004444 -0.687583  ...  0.012733 -0.030278  0.068182   \n",
       "\n",
       "           94        95        96        97        98        99  target  \n",
       "0    0.010857 -0.000154  0.003283  0.007735 -0.065010 -0.028622       2  \n",
       "1    0.013084  0.000544  0.045964  0.032976 -0.035506  0.002295       2  \n",
       "2    0.120064 -0.046621 -0.022233 -0.020058  0.048230  0.007981       2  \n",
       "3   -0.050769 -0.038103 -0.014820 -0.022481  0.027135 -0.036524       2  \n",
       "4    0.029601  0.016971 -0.001190  0.047220 -0.000497 -0.024342       2  \n",
       "..        ...       ...       ...       ...       ...       ...     ...  \n",
       "923 -0.031645  0.009314 -0.022128 -0.049114 -0.007692  0.014365       3  \n",
       "924 -0.028358 -0.009963  0.057517 -0.007851 -0.063690 -0.020377       3  \n",
       "925  0.005146 -0.022885  0.069912  0.021681 -0.007789 -0.010856       3  \n",
       "926 -0.078990 -0.005638  0.035686 -0.056002  0.018786  0.063739       3  \n",
       "927 -0.062381  0.014018 -0.072341  0.007953 -0.053482 -0.029433       3  \n",
       "\n",
       "[928 rows x 101 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just for testing\n",
    "# Now Perform Dimensionality reduction (PCA) on that Dataframe and check\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#do PCA and choose componeents as 100\n",
    "pca = PCA(n_components=100)\n",
    "x_pca = pca.fit_transform(test_final.iloc[:,0:-1])\n",
    "x_pca = pd.DataFrame(x_pca)\n",
    "\n",
    "# Calculate the variance explained by priciple components\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print('Variance of each component:', pca.explained_variance_ratio_)\n",
    "print('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))\n",
    "\n",
    "#store the new pca generated dimensions in a dataframe\n",
    "pca_df = pd.DataFrame(data = x_pca)\n",
    "target = pd.Series(test_final['target'], name='target')\n",
    "result_df = pd.concat([pca_df, target], axis=1)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "ErMVKLMrKBiJ",
    "outputId": "f7ccdbdd-773b-4a0e-d7de-d467d120a211"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.018578</td>\n",
       "      <td>1.148263</td>\n",
       "      <td>-0.589582</td>\n",
       "      <td>0.193617</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>-0.309400</td>\n",
       "      <td>-0.161566</td>\n",
       "      <td>0.478471</td>\n",
       "      <td>0.972403</td>\n",
       "      <td>-0.031832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012254</td>\n",
       "      <td>-0.003815</td>\n",
       "      <td>0.025192</td>\n",
       "      <td>0.010857</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>0.003283</td>\n",
       "      <td>0.007735</td>\n",
       "      <td>-0.065010</td>\n",
       "      <td>-0.028622</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.098692</td>\n",
       "      <td>0.289832</td>\n",
       "      <td>-1.766388</td>\n",
       "      <td>1.076165</td>\n",
       "      <td>-0.261201</td>\n",
       "      <td>-0.820446</td>\n",
       "      <td>-0.474188</td>\n",
       "      <td>-0.515238</td>\n",
       "      <td>0.692389</td>\n",
       "      <td>1.501606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017533</td>\n",
       "      <td>0.060498</td>\n",
       "      <td>-0.007995</td>\n",
       "      <td>0.013084</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.045964</td>\n",
       "      <td>0.032976</td>\n",
       "      <td>-0.035506</td>\n",
       "      <td>0.002295</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.275021</td>\n",
       "      <td>-0.451289</td>\n",
       "      <td>0.106750</td>\n",
       "      <td>-0.426415</td>\n",
       "      <td>0.066133</td>\n",
       "      <td>0.692474</td>\n",
       "      <td>0.634894</td>\n",
       "      <td>-0.035867</td>\n",
       "      <td>0.815855</td>\n",
       "      <td>-0.909473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045105</td>\n",
       "      <td>-0.042407</td>\n",
       "      <td>-0.028622</td>\n",
       "      <td>0.120064</td>\n",
       "      <td>-0.046621</td>\n",
       "      <td>-0.022233</td>\n",
       "      <td>-0.020058</td>\n",
       "      <td>0.048230</td>\n",
       "      <td>0.007981</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.517085</td>\n",
       "      <td>1.662693</td>\n",
       "      <td>-1.021167</td>\n",
       "      <td>0.804267</td>\n",
       "      <td>-0.281985</td>\n",
       "      <td>0.518180</td>\n",
       "      <td>0.355748</td>\n",
       "      <td>-0.344235</td>\n",
       "      <td>-0.910867</td>\n",
       "      <td>-0.629517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.042739</td>\n",
       "      <td>0.015855</td>\n",
       "      <td>-0.050769</td>\n",
       "      <td>-0.038103</td>\n",
       "      <td>-0.014820</td>\n",
       "      <td>-0.022481</td>\n",
       "      <td>0.027135</td>\n",
       "      <td>-0.036524</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.152840</td>\n",
       "      <td>-1.046283</td>\n",
       "      <td>0.351278</td>\n",
       "      <td>1.100381</td>\n",
       "      <td>-1.613642</td>\n",
       "      <td>1.484188</td>\n",
       "      <td>-0.113277</td>\n",
       "      <td>-0.251152</td>\n",
       "      <td>0.179023</td>\n",
       "      <td>-0.233104</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008807</td>\n",
       "      <td>0.054492</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.029601</td>\n",
       "      <td>0.016971</td>\n",
       "      <td>-0.001190</td>\n",
       "      <td>0.047220</td>\n",
       "      <td>-0.000497</td>\n",
       "      <td>-0.024342</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>-1.321884</td>\n",
       "      <td>2.153021</td>\n",
       "      <td>0.788596</td>\n",
       "      <td>-1.304253</td>\n",
       "      <td>0.458186</td>\n",
       "      <td>-0.859346</td>\n",
       "      <td>-0.069127</td>\n",
       "      <td>-0.392796</td>\n",
       "      <td>0.755732</td>\n",
       "      <td>-0.584050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037369</td>\n",
       "      <td>0.077176</td>\n",
       "      <td>-0.044731</td>\n",
       "      <td>-0.031645</td>\n",
       "      <td>0.009314</td>\n",
       "      <td>-0.022128</td>\n",
       "      <td>-0.049114</td>\n",
       "      <td>-0.007692</td>\n",
       "      <td>0.014365</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>-0.867163</td>\n",
       "      <td>-0.040504</td>\n",
       "      <td>0.940680</td>\n",
       "      <td>0.302648</td>\n",
       "      <td>-0.469672</td>\n",
       "      <td>-0.368255</td>\n",
       "      <td>1.065579</td>\n",
       "      <td>0.801522</td>\n",
       "      <td>0.690113</td>\n",
       "      <td>0.953008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012492</td>\n",
       "      <td>-0.093375</td>\n",
       "      <td>0.014933</td>\n",
       "      <td>-0.028358</td>\n",
       "      <td>-0.009963</td>\n",
       "      <td>0.057517</td>\n",
       "      <td>-0.007851</td>\n",
       "      <td>-0.063690</td>\n",
       "      <td>-0.020377</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>3.753012</td>\n",
       "      <td>0.841636</td>\n",
       "      <td>-0.317393</td>\n",
       "      <td>-0.296117</td>\n",
       "      <td>0.593769</td>\n",
       "      <td>-0.255474</td>\n",
       "      <td>-0.057091</td>\n",
       "      <td>-0.072048</td>\n",
       "      <td>0.664386</td>\n",
       "      <td>-0.837668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008571</td>\n",
       "      <td>-0.006161</td>\n",
       "      <td>-0.003945</td>\n",
       "      <td>0.005146</td>\n",
       "      <td>-0.022885</td>\n",
       "      <td>0.069912</td>\n",
       "      <td>0.021681</td>\n",
       "      <td>-0.007789</td>\n",
       "      <td>-0.010856</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>0.603083</td>\n",
       "      <td>0.126259</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>0.283612</td>\n",
       "      <td>0.169559</td>\n",
       "      <td>-0.156326</td>\n",
       "      <td>-0.068399</td>\n",
       "      <td>-0.184308</td>\n",
       "      <td>0.461063</td>\n",
       "      <td>-0.002047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056376</td>\n",
       "      <td>0.024160</td>\n",
       "      <td>-0.015827</td>\n",
       "      <td>-0.078990</td>\n",
       "      <td>-0.005638</td>\n",
       "      <td>0.035686</td>\n",
       "      <td>-0.056002</td>\n",
       "      <td>0.018786</td>\n",
       "      <td>0.063739</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>-1.452945</td>\n",
       "      <td>1.233599</td>\n",
       "      <td>0.439472</td>\n",
       "      <td>0.278517</td>\n",
       "      <td>0.165928</td>\n",
       "      <td>-0.171830</td>\n",
       "      <td>-0.075000</td>\n",
       "      <td>0.033859</td>\n",
       "      <td>0.004444</td>\n",
       "      <td>-0.687583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012733</td>\n",
       "      <td>-0.030278</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>-0.062381</td>\n",
       "      <td>0.014018</td>\n",
       "      <td>-0.072341</td>\n",
       "      <td>0.007953</td>\n",
       "      <td>-0.053482</td>\n",
       "      <td>-0.029433</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    1.018578  1.148263 -0.589582  0.193617  0.047950 -0.309400 -0.161566   \n",
       "1   -1.098692  0.289832 -1.766388  1.076165 -0.261201 -0.820446 -0.474188   \n",
       "2    0.275021 -0.451289  0.106750 -0.426415  0.066133  0.692474  0.634894   \n",
       "3   -1.517085  1.662693 -1.021167  0.804267 -0.281985  0.518180  0.355748   \n",
       "4   -0.152840 -1.046283  0.351278  1.100381 -1.613642  1.484188 -0.113277   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "923 -1.321884  2.153021  0.788596 -1.304253  0.458186 -0.859346 -0.069127   \n",
       "924 -0.867163 -0.040504  0.940680  0.302648 -0.469672 -0.368255  1.065579   \n",
       "925  3.753012  0.841636 -0.317393 -0.296117  0.593769 -0.255474 -0.057091   \n",
       "926  0.603083  0.126259  0.003433  0.283612  0.169559 -0.156326 -0.068399   \n",
       "927 -1.452945  1.233599  0.439472  0.278517  0.165928 -0.171830 -0.075000   \n",
       "\n",
       "            7         8         9  ...        91        92        93  \\\n",
       "0    0.478471  0.972403 -0.031832  ...  0.012254 -0.003815  0.025192   \n",
       "1   -0.515238  0.692389  1.501606  ... -0.017533  0.060498 -0.007995   \n",
       "2   -0.035867  0.815855 -0.909473  ...  0.045105 -0.042407 -0.028622   \n",
       "3   -0.344235 -0.910867 -0.629517  ...  0.000802  0.042739  0.015855   \n",
       "4   -0.251152  0.179023 -0.233104  ... -0.008807  0.054492  0.001487   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "923 -0.392796  0.755732 -0.584050  ... -0.037369  0.077176 -0.044731   \n",
       "924  0.801522  0.690113  0.953008  ...  0.012492 -0.093375  0.014933   \n",
       "925 -0.072048  0.664386 -0.837668  ... -0.008571 -0.006161 -0.003945   \n",
       "926 -0.184308  0.461063 -0.002047  ...  0.056376  0.024160 -0.015827   \n",
       "927  0.033859  0.004444 -0.687583  ...  0.012733 -0.030278  0.068182   \n",
       "\n",
       "           94        95        96        97        98        99  target  \n",
       "0    0.010857 -0.000154  0.003283  0.007735 -0.065010 -0.028622       2  \n",
       "1    0.013084  0.000544  0.045964  0.032976 -0.035506  0.002295       2  \n",
       "2    0.120064 -0.046621 -0.022233 -0.020058  0.048230  0.007981       2  \n",
       "3   -0.050769 -0.038103 -0.014820 -0.022481  0.027135 -0.036524       2  \n",
       "4    0.029601  0.016971 -0.001190  0.047220 -0.000497 -0.024342       2  \n",
       "..        ...       ...       ...       ...       ...       ...     ...  \n",
       "923 -0.031645  0.009314 -0.022128 -0.049114 -0.007692  0.014365       3  \n",
       "924 -0.028358 -0.009963  0.057517 -0.007851 -0.063690 -0.020377       3  \n",
       "925  0.005146 -0.022885  0.069912  0.021681 -0.007789 -0.010856       3  \n",
       "926 -0.078990 -0.005638  0.035686 -0.056002  0.018786  0.063739       3  \n",
       "927 -0.062381  0.014018 -0.072341  0.007953 -0.053482 -0.029433       3  \n",
       "\n",
       "[928 rows x 101 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UOI2lH6iOIY"
   },
   "source": [
    "#### **TRYING DIFFERENT ML MODELS ON A SINGLE LEAD(EX : 1) POST DIMENSIONALITY REDUCTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rE3l8LNQizo0"
   },
   "source": [
    "##### **KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Me4jqP4Zih_I",
    "outputId": "83dde896-7166-44cb-b88f-725eaedb9d34"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:824: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 813, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 753, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 705, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 246, in predict\n",
      "    if self._fit_method == \"brute\" and ArgKminClassMode.is_usable_for(\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 471, in is_usable_for\n",
      "    ArgKmin.is_usable_for(X, Y, metric)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 115, in is_usable_for\n",
      "    and (is_numpy_c_ordered(X) or is_valid_sparse_matrix(X))\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 99, in is_numpy_c_ordered\n",
      "    return hasattr(X, \"flags\") and X.flags.c_contiguous\n",
      "AttributeError: 'Flags' object has no attribute 'c_contiguous'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:824: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 813, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 753, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 705, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 246, in predict\n",
      "    if self._fit_method == \"brute\" and ArgKminClassMode.is_usable_for(\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 471, in is_usable_for\n",
      "    ArgKmin.is_usable_for(X, Y, metric)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 115, in is_usable_for\n",
      "    and (is_numpy_c_ordered(X) or is_valid_sparse_matrix(X))\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 99, in is_numpy_c_ordered\n",
      "    return hasattr(X, \"flags\") and X.flags.c_contiguous\n",
      "AttributeError: 'Flags' object has no attribute 'c_contiguous'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:824: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 813, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 753, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 705, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 246, in predict\n",
      "    if self._fit_method == \"brute\" and ArgKminClassMode.is_usable_for(\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 471, in is_usable_for\n",
      "    ArgKmin.is_usable_for(X, Y, metric)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 115, in is_usable_for\n",
      "    and (is_numpy_c_ordered(X) or is_valid_sparse_matrix(X))\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 99, in is_numpy_c_ordered\n",
      "    return hasattr(X, \"flags\") and X.flags.c_contiguous\n",
      "AttributeError: 'Flags' object has no attribute 'c_contiguous'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:824: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 813, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 753, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 705, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 246, in predict\n",
      "    if self._fit_method == \"brute\" and ArgKminClassMode.is_usable_for(\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 471, in is_usable_for\n",
      "    ArgKmin.is_usable_for(X, Y, metric)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 115, in is_usable_for\n",
      "    and (is_numpy_c_ordered(X) or is_valid_sparse_matrix(X))\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 99, in is_numpy_c_ordered\n",
      "    return hasattr(X, \"flags\") and X.flags.c_contiguous\n",
      "AttributeError: 'Flags' object has no attribute 'c_contiguous'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:824: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 813, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 753, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 705, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 246, in predict\n",
      "    if self._fit_method == \"brute\" and ArgKminClassMode.is_usable_for(\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 471, in is_usable_for\n",
      "    ArgKmin.is_usable_for(X, Y, metric)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 115, in is_usable_for\n",
      "    and (is_numpy_c_ordered(X) or is_valid_sparse_matrix(X))\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 99, in is_numpy_c_ordered\n",
      "    return hasattr(X, \"flags\") and X.flags.c_contiguous\n",
      "AttributeError: 'Flags' object has no attribute 'c_contiguous'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:824: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 813, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 753, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 705, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 246, in predict\n",
      "    if self._fit_method == \"brute\" and ArgKminClassMode.is_usable_for(\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 471, in is_usable_for\n",
      "    ArgKmin.is_usable_for(X, Y, metric)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 115, in is_usable_for\n",
      "    and (is_numpy_c_ordered(X) or is_valid_sparse_matrix(X))\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 99, in is_numpy_c_ordered\n",
      "    return hasattr(X, \"flags\") and X.flags.c_contiguous\n",
      "AttributeError: 'Flags' object has no attribute 'c_contiguous'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:824: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 813, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 753, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 705, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 246, in predict\n",
      "    if self._fit_method == \"brute\" and ArgKminClassMode.is_usable_for(\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 471, in is_usable_for\n",
      "    ArgKmin.is_usable_for(X, Y, metric)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 115, in is_usable_for\n",
      "    and (is_numpy_c_ordered(X) or is_valid_sparse_matrix(X))\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 99, in is_numpy_c_ordered\n",
      "    return hasattr(X, \"flags\") and X.flags.c_contiguous\n",
      "AttributeError: 'Flags' object has no attribute 'c_contiguous'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:824: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 813, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 753, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 705, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 246, in predict\n",
      "    if self._fit_method == \"brute\" and ArgKminClassMode.is_usable_for(\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 471, in is_usable_for\n",
      "    ArgKmin.is_usable_for(X, Y, metric)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 115, in is_usable_for\n",
      "    and (is_numpy_c_ordered(X) or is_valid_sparse_matrix(X))\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 99, in is_numpy_c_ordered\n",
      "    return hasattr(X, \"flags\") and X.flags.c_contiguous\n",
      "AttributeError: 'Flags' object has no attribute 'c_contiguous'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:824: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 813, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 753, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 705, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 246, in predict\n",
      "    if self._fit_method == \"brute\" and ArgKminClassMode.is_usable_for(\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 471, in is_usable_for\n",
      "    ArgKmin.is_usable_for(X, Y, metric)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 115, in is_usable_for\n",
      "    and (is_numpy_c_ordered(X) or is_valid_sparse_matrix(X))\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 99, in is_numpy_c_ordered\n",
      "    return hasattr(X, \"flags\") and X.flags.c_contiguous\n",
      "AttributeError: 'Flags' object has no attribute 'c_contiguous'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:824: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 813, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 753, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 705, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 246, in predict\n",
      "    if self._fit_method == \"brute\" and ArgKminClassMode.is_usable_for(\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 471, in is_usable_for\n",
      "    ArgKmin.is_usable_for(X, Y, metric)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 115, in is_usable_for\n",
      "    and (is_numpy_c_ordered(X) or is_valid_sparse_matrix(X))\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 99, in is_numpy_c_ordered\n",
      "    return hasattr(X, \"flags\") and X.flags.c_contiguous\n",
      "AttributeError: 'Flags' object has no attribute 'c_contiguous'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:824: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 813, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 753, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 705, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 246, in predict\n",
      "    if self._fit_method == \"brute\" and ArgKminClassMode.is_usable_for(\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 471, in is_usable_for\n",
      "    ArgKmin.is_usable_for(X, Y, metric)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 115, in is_usable_for\n",
      "    and (is_numpy_c_ordered(X) or is_valid_sparse_matrix(X))\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 99, in is_numpy_c_ordered\n",
      "    return hasattr(X, \"flags\") and X.flags.c_contiguous\n",
      "AttributeError: 'Flags' object has no attribute 'c_contiguous'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:824: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 813, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 753, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 705, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 246, in predict\n",
      "    if self._fit_method == \"brute\" and ArgKminClassMode.is_usable_for(\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 471, in is_usable_for\n",
      "    ArgKmin.is_usable_for(X, Y, metric)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 115, in is_usable_for\n",
      "    and (is_numpy_c_ordered(X) or is_valid_sparse_matrix(X))\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 99, in is_numpy_c_ordered\n",
      "    return hasattr(X, \"flags\") and X.flags.c_contiguous\n",
      "AttributeError: 'Flags' object has no attribute 'c_contiguous'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:824: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 813, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 753, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 705, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 246, in predict\n",
      "    if self._fit_method == \"brute\" and ArgKminClassMode.is_usable_for(\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 471, in is_usable_for\n",
      "    ArgKmin.is_usable_for(X, Y, metric)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 115, in is_usable_for\n",
      "    and (is_numpy_c_ordered(X) or is_valid_sparse_matrix(X))\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 99, in is_numpy_c_ordered\n",
      "    return hasattr(X, \"flags\") and X.flags.c_contiguous\n",
      "AttributeError: 'Flags' object has no attribute 'c_contiguous'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:824: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 813, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 753, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 705, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 246, in predict\n",
      "    if self._fit_method == \"brute\" and ArgKminClassMode.is_usable_for(\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 471, in is_usable_for\n",
      "    ArgKmin.is_usable_for(X, Y, metric)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 115, in is_usable_for\n",
      "    and (is_numpy_c_ordered(X) or is_valid_sparse_matrix(X))\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 99, in is_numpy_c_ordered\n",
      "    return hasattr(X, \"flags\") and X.flags.c_contiguous\n",
      "AttributeError: 'Flags' object has no attribute 'c_contiguous'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:824: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 813, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 753, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 705, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 246, in predict\n",
      "    if self._fit_method == \"brute\" and ArgKminClassMode.is_usable_for(\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 471, in is_usable_for\n",
      "    ArgKmin.is_usable_for(X, Y, metric)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 115, in is_usable_for\n",
      "    and (is_numpy_c_ordered(X) or is_valid_sparse_matrix(X))\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 99, in is_numpy_c_ordered\n",
      "    return hasattr(X, \"flags\") and X.flags.c_contiguous\n",
      "AttributeError: 'Flags' object has no attribute 'c_contiguous'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:824: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 813, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 527, in __call__\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py\", line 753, in score\n",
      "    return self.steps[-1][1].score(Xt, y, **score_params)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 705, in score\n",
      "    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 246, in predict\n",
      "    if self._fit_method == \"brute\" and ArgKminClassMode.is_usable_for(\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 471, in is_usable_for\n",
      "    ArgKmin.is_usable_for(X, Y, metric)\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 115, in is_usable_for\n",
      "    and (is_numpy_c_ordered(X) or is_valid_sparse_matrix(X))\n",
      "  File \"C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py\", line 99, in is_numpy_c_ordered\n",
      "    return hasattr(X, \"flags\") and X.flags.c_contiguous\n",
      "AttributeError: 'Flags' object has no attribute 'c_contiguous'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\prabh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Flags' object has no attribute 'c_contiguous'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m cv\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Predict the labels of the test set\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Accuracy\u001b[39;00m\n\u001b[0;32m     35\u001b[0m Knn_Accuracy \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mscore(X_test, y_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:519\u001b[0m, in \u001b[0;36mBaseSearchCV.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call predict on the estimator with the best found parameters.\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \n\u001b[0;32m    503\u001b[0m \u001b[38;5;124;03mOnly available if ``refit=True`` and the underlying estimator supports\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;124;03m    the best found parameters.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    518\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 519\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:508\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    507\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform(Xt)\n\u001b[1;32m--> 508\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_params)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:246\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    244\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fit_method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mArgKminClassMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_usable_for\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    249\u001b[0m         probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_proba(X)\n\u001b[0;32m    250\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_2d_:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:471\u001b[0m, in \u001b[0;36mArgKminClassMode.is_usable_for\u001b[1;34m(cls, X, Y, metric)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_usable_for\u001b[39m(\u001b[38;5;28mcls\u001b[39m, X, Y, metric) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    450\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return True if the dispatcher can be used for the given parameters.\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m    True if the PairwiseDistancesReduction can be used, else False.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 471\u001b[0m         \u001b[43mArgKmin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_usable_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    472\u001b[0m         \u001b[38;5;66;03m# TODO: Support CSR matrices.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(X)\n\u001b[0;32m    474\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(Y)\n\u001b[0;32m    475\u001b[0m         \u001b[38;5;66;03m# TODO: implement Euclidean specialization with GEMM.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m metric \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqeuclidean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    477\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:115\u001b[0m, in \u001b[0;36mBaseDistancesReductionDispatcher.is_usable_for\u001b[1;34m(cls, X, Y, metric)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_valid_sparse_matrix\u001b[39m(X):\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    103\u001b[0m         isspmatrix_csr(X)\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    110\u001b[0m         X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mint32\n\u001b[0;32m    111\u001b[0m     )\n\u001b[0;32m    113\u001b[0m is_usable \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    114\u001b[0m     get_config()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable_cython_pairwise_dist\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[43mis_numpy_c_ordered\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m is_valid_sparse_matrix(X))\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (is_numpy_c_ordered(Y) \u001b[38;5;129;01mor\u001b[39;00m is_valid_sparse_matrix(Y))\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m (np\u001b[38;5;241m.\u001b[39mfloat32, np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_metrics()\n\u001b[0;32m    120\u001b[0m )\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_usable\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:99\u001b[0m, in \u001b[0;36mBaseDistancesReductionDispatcher.is_usable_for.<locals>.is_numpy_c_ordered\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_numpy_c_ordered\u001b[39m(X):\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflags\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_contiguous\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Flags' object has no attribute 'c_contiguous'"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline steps:\n",
    "steps = [('knn', KNeighborsClassifier())]\n",
    "        \n",
    "# Create the pipeline: pipeline \n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# have paased less range value of hyperparamter since i'm using free tier version of google colab.\n",
    "k_range = list(range(1, 9))\n",
    "parameters = dict(knn__n_neighbors=k_range)\n",
    "\n",
    "#input\n",
    "X = result_df.iloc[:,0:-1]\n",
    "\n",
    "#target\n",
    "y=result_df.iloc[:,-1]\n",
    "\n",
    "# Create train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "\n",
    "#increasing cv score takes lot of time in gooogle colab, so kept it just 2.\n",
    "cv = GridSearchCV(pipeline,parameters,cv=2)\n",
    "\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "Knn_Accuracy = cv.score(X_test, y_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(Knn_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDkFlXChia-8"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKtMbgXLifbX"
   },
   "source": [
    "##### **LOGISTIC REGRESSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BbGt_eOVkJq6"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline steps:\n",
    "steps = [('lr', LogisticRegression())]\n",
    "        \n",
    "# Create the pipeline: pipeline \n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "#input\n",
    "X = result_df.iloc[:,0:-1]\n",
    "\n",
    "#target\n",
    "y=result_df.iloc[:,-1]\n",
    "\n",
    "#parameters for gridsearchcv if we increase range of entries from 5 to higher value, we can get greater accurange\n",
    "c_space = np.logspace(-4, 4, 3)\n",
    "parameters = {'lr__C': c_space,'lr__penalty': ['l2']} \n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "\n",
    "#call GridSearchCV and set crossvalscore to 2\n",
    "cv = GridSearchCV(pipeline,parameters,cv=2)\n",
    "\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = cv.predict(X_test)\n",
    "LR_Accuracy = cv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3k9i12eGunX1",
    "outputId": "21ce6e7f-653e-4710-d585-ff445736e08b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.543010752688172\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.33      0.35       105\n",
      "           1       0.73      0.91      0.81        94\n",
      "           2       0.56      0.58      0.57       112\n",
      "           3       0.38      0.26      0.31        61\n",
      "\n",
      "    accuracy                           0.54       372\n",
      "   macro avg       0.51      0.52      0.51       372\n",
      "weighted avg       0.52      0.54      0.53       372\n",
      "\n",
      "Tuned Model Parameters: {'lr__C': 10000.0, 'lr__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(LR_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_W0sQQDwiisE"
   },
   "source": [
    "##### **SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94xczE1iurC9",
    "outputId": "d5530fa8-865f-4ec0-80c9-f89940866c43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8225806451612904\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      1.00      0.74        93\n",
      "           1       1.00      1.00      1.00        99\n",
      "           2       1.00      0.61      0.76       117\n",
      "           3       1.00      0.68      0.81        63\n",
      "\n",
      "    accuracy                           0.82       372\n",
      "   macro avg       0.90      0.82      0.83       372\n",
      "weighted avg       0.90      0.82      0.83       372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline\n",
    "steps = [('SVM', SVC())]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "#input\n",
    "X = result_df.iloc[:,0:-1]\n",
    "\n",
    "#target\n",
    "y=result_df.iloc[:,-1]\n",
    "\n",
    "# Specify the hyperparameter space, if we increase the penalty(c) and gamma value the accurancy can be increased.\n",
    "#since it takes lots of time in google colab provided only a single value\n",
    "parameters = {'SVM__C':[10],'SVM__gamma':[1]}\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=21)\n",
    "\n",
    "cv = GridSearchCV(pipeline,parameters,cv=3)\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "SVM_Accuracy = cv.score(X_test, y_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "SVM_Accuracy=cv.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy: {}\".format(SVM_Accuracy))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1GNZtOUi6PP"
   },
   "source": [
    "#### **NOW COMBINING ALL 12 LEADS INTO A SINGLE CSV FILE AND THEN PERFROM MODEL ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "huNy0hsWSkr5",
    "outputId": "ae4fc5c3-8b46-4d0a-d075-c2c64559680c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>3021</th>\n",
       "      <th>3022</th>\n",
       "      <th>3023</th>\n",
       "      <th>3024</th>\n",
       "      <th>3025</th>\n",
       "      <th>3026</th>\n",
       "      <th>3027</th>\n",
       "      <th>3028</th>\n",
       "      <th>3029</th>\n",
       "      <th>3030</th>\n",
       "      <th>3031</th>\n",
       "      <th>3032</th>\n",
       "      <th>3033</th>\n",
       "      <th>3034</th>\n",
       "      <th>3035</th>\n",
       "      <th>3036</th>\n",
       "      <th>3037</th>\n",
       "      <th>3038</th>\n",
       "      <th>3039</th>\n",
       "      <th>3040</th>\n",
       "      <th>3041</th>\n",
       "      <th>3042</th>\n",
       "      <th>3043</th>\n",
       "      <th>3044</th>\n",
       "      <th>3045</th>\n",
       "      <th>3046</th>\n",
       "      <th>3047</th>\n",
       "      <th>3048</th>\n",
       "      <th>3049</th>\n",
       "      <th>3050</th>\n",
       "      <th>3051</th>\n",
       "      <th>3052</th>\n",
       "      <th>3053</th>\n",
       "      <th>3054</th>\n",
       "      <th>3055</th>\n",
       "      <th>3056</th>\n",
       "      <th>3057</th>\n",
       "      <th>3058</th>\n",
       "      <th>3059</th>\n",
       "      <th>3060</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.728449</td>\n",
       "      <td>0.680755</td>\n",
       "      <td>0.619010</td>\n",
       "      <td>0.645367</td>\n",
       "      <td>0.681570</td>\n",
       "      <td>0.732488</td>\n",
       "      <td>0.758448</td>\n",
       "      <td>0.750660</td>\n",
       "      <td>0.728282</td>\n",
       "      <td>0.707928</td>\n",
       "      <td>0.710137</td>\n",
       "      <td>0.722235</td>\n",
       "      <td>0.706999</td>\n",
       "      <td>0.706216</td>\n",
       "      <td>0.706171</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>0.659254</td>\n",
       "      <td>0.593152</td>\n",
       "      <td>0.519543</td>\n",
       "      <td>0.422848</td>\n",
       "      <td>0.343645</td>\n",
       "      <td>0.323963</td>\n",
       "      <td>0.390536</td>\n",
       "      <td>0.459191</td>\n",
       "      <td>0.519838</td>\n",
       "      <td>0.582817</td>\n",
       "      <td>0.631090</td>\n",
       "      <td>0.668859</td>\n",
       "      <td>0.696149</td>\n",
       "      <td>0.739028</td>\n",
       "      <td>0.863386</td>\n",
       "      <td>0.948694</td>\n",
       "      <td>0.894116</td>\n",
       "      <td>0.752051</td>\n",
       "      <td>0.613568</td>\n",
       "      <td>0.466687</td>\n",
       "      <td>0.449288</td>\n",
       "      <td>0.580020</td>\n",
       "      <td>0.711661</td>\n",
       "      <td>0.802473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.745040</td>\n",
       "      <td>0.702334</td>\n",
       "      <td>0.720247</td>\n",
       "      <td>0.768276</td>\n",
       "      <td>0.818492</td>\n",
       "      <td>0.862429</td>\n",
       "      <td>0.889854</td>\n",
       "      <td>0.910468</td>\n",
       "      <td>0.912257</td>\n",
       "      <td>0.947332</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962571</td>\n",
       "      <td>0.873184</td>\n",
       "      <td>0.780943</td>\n",
       "      <td>0.688698</td>\n",
       "      <td>0.596477</td>\n",
       "      <td>0.509179</td>\n",
       "      <td>0.488464</td>\n",
       "      <td>0.568285</td>\n",
       "      <td>0.653875</td>\n",
       "      <td>0.740760</td>\n",
       "      <td>0.824287</td>\n",
       "      <td>0.904260</td>\n",
       "      <td>0.945590</td>\n",
       "      <td>0.913475</td>\n",
       "      <td>0.897487</td>\n",
       "      <td>0.864779</td>\n",
       "      <td>0.830106</td>\n",
       "      <td>0.841002</td>\n",
       "      <td>0.865206</td>\n",
       "      <td>0.864067</td>\n",
       "      <td>0.849256</td>\n",
       "      <td>0.854949</td>\n",
       "      <td>0.861380</td>\n",
       "      <td>0.875514</td>\n",
       "      <td>0.868763</td>\n",
       "      <td>0.847450</td>\n",
       "      <td>0.805689</td>\n",
       "      <td>0.751761</td>\n",
       "      <td>0.702102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.957972</td>\n",
       "      <td>0.950695</td>\n",
       "      <td>0.941024</td>\n",
       "      <td>0.930501</td>\n",
       "      <td>0.913601</td>\n",
       "      <td>0.892244</td>\n",
       "      <td>0.868016</td>\n",
       "      <td>0.855127</td>\n",
       "      <td>0.835307</td>\n",
       "      <td>0.798640</td>\n",
       "      <td>0.733188</td>\n",
       "      <td>0.664062</td>\n",
       "      <td>0.601404</td>\n",
       "      <td>0.595782</td>\n",
       "      <td>0.646351</td>\n",
       "      <td>0.703396</td>\n",
       "      <td>0.761126</td>\n",
       "      <td>0.802921</td>\n",
       "      <td>0.836469</td>\n",
       "      <td>0.858247</td>\n",
       "      <td>0.864279</td>\n",
       "      <td>0.852347</td>\n",
       "      <td>0.802839</td>\n",
       "      <td>0.719658</td>\n",
       "      <td>0.620187</td>\n",
       "      <td>0.517033</td>\n",
       "      <td>0.422197</td>\n",
       "      <td>0.363759</td>\n",
       "      <td>0.425602</td>\n",
       "      <td>0.525786</td>\n",
       "      <td>0.620086</td>\n",
       "      <td>0.715116</td>\n",
       "      <td>0.793895</td>\n",
       "      <td>0.880380</td>\n",
       "      <td>0.942970</td>\n",
       "      <td>0.958189</td>\n",
       "      <td>0.978297</td>\n",
       "      <td>0.964475</td>\n",
       "      <td>0.953444</td>\n",
       "      <td>0.982407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563454</td>\n",
       "      <td>0.497534</td>\n",
       "      <td>0.510498</td>\n",
       "      <td>0.578354</td>\n",
       "      <td>0.649225</td>\n",
       "      <td>0.713104</td>\n",
       "      <td>0.772009</td>\n",
       "      <td>0.824100</td>\n",
       "      <td>0.860400</td>\n",
       "      <td>0.885179</td>\n",
       "      <td>0.929594</td>\n",
       "      <td>0.987471</td>\n",
       "      <td>0.964358</td>\n",
       "      <td>0.880329</td>\n",
       "      <td>0.792713</td>\n",
       "      <td>0.698901</td>\n",
       "      <td>0.607952</td>\n",
       "      <td>0.518268</td>\n",
       "      <td>0.423372</td>\n",
       "      <td>0.337925</td>\n",
       "      <td>0.354889</td>\n",
       "      <td>0.446820</td>\n",
       "      <td>0.541939</td>\n",
       "      <td>0.636193</td>\n",
       "      <td>0.724207</td>\n",
       "      <td>0.816468</td>\n",
       "      <td>0.902898</td>\n",
       "      <td>0.970226</td>\n",
       "      <td>0.982677</td>\n",
       "      <td>0.962885</td>\n",
       "      <td>0.925865</td>\n",
       "      <td>0.928285</td>\n",
       "      <td>0.946033</td>\n",
       "      <td>0.947274</td>\n",
       "      <td>0.946394</td>\n",
       "      <td>0.936536</td>\n",
       "      <td>0.920869</td>\n",
       "      <td>0.910320</td>\n",
       "      <td>0.905436</td>\n",
       "      <td>0.876942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.611084</td>\n",
       "      <td>0.661575</td>\n",
       "      <td>0.695790</td>\n",
       "      <td>0.741113</td>\n",
       "      <td>0.716666</td>\n",
       "      <td>0.595794</td>\n",
       "      <td>0.425022</td>\n",
       "      <td>0.286457</td>\n",
       "      <td>0.425022</td>\n",
       "      <td>0.611384</td>\n",
       "      <td>0.775065</td>\n",
       "      <td>0.847613</td>\n",
       "      <td>0.862143</td>\n",
       "      <td>0.886017</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.903664</td>\n",
       "      <td>0.946845</td>\n",
       "      <td>0.958480</td>\n",
       "      <td>0.871704</td>\n",
       "      <td>0.750220</td>\n",
       "      <td>0.589334</td>\n",
       "      <td>0.548793</td>\n",
       "      <td>0.550431</td>\n",
       "      <td>0.651875</td>\n",
       "      <td>0.759864</td>\n",
       "      <td>0.835307</td>\n",
       "      <td>0.860687</td>\n",
       "      <td>0.843376</td>\n",
       "      <td>0.820468</td>\n",
       "      <td>0.815694</td>\n",
       "      <td>0.805792</td>\n",
       "      <td>0.829913</td>\n",
       "      <td>0.836955</td>\n",
       "      <td>0.819891</td>\n",
       "      <td>0.799291</td>\n",
       "      <td>0.755413</td>\n",
       "      <td>0.701184</td>\n",
       "      <td>0.642225</td>\n",
       "      <td>0.591050</td>\n",
       "      <td>0.579384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.579605</td>\n",
       "      <td>0.722498</td>\n",
       "      <td>0.851734</td>\n",
       "      <td>0.943952</td>\n",
       "      <td>0.890738</td>\n",
       "      <td>0.769205</td>\n",
       "      <td>0.641047</td>\n",
       "      <td>0.509940</td>\n",
       "      <td>0.394096</td>\n",
       "      <td>0.370311</td>\n",
       "      <td>0.383544</td>\n",
       "      <td>0.386751</td>\n",
       "      <td>0.394932</td>\n",
       "      <td>0.407042</td>\n",
       "      <td>0.375626</td>\n",
       "      <td>0.314456</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.237970</td>\n",
       "      <td>0.314892</td>\n",
       "      <td>0.363607</td>\n",
       "      <td>0.385089</td>\n",
       "      <td>0.391721</td>\n",
       "      <td>0.386943</td>\n",
       "      <td>0.383401</td>\n",
       "      <td>0.379440</td>\n",
       "      <td>0.391886</td>\n",
       "      <td>0.388549</td>\n",
       "      <td>0.359400</td>\n",
       "      <td>0.311161</td>\n",
       "      <td>0.235529</td>\n",
       "      <td>0.170137</td>\n",
       "      <td>0.166206</td>\n",
       "      <td>0.207633</td>\n",
       "      <td>0.258184</td>\n",
       "      <td>0.286993</td>\n",
       "      <td>0.304742</td>\n",
       "      <td>0.325659</td>\n",
       "      <td>0.361189</td>\n",
       "      <td>0.451946</td>\n",
       "      <td>0.543373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.839213</td>\n",
       "      <td>0.861690</td>\n",
       "      <td>0.866457</td>\n",
       "      <td>0.865756</td>\n",
       "      <td>0.855027</td>\n",
       "      <td>0.855606</td>\n",
       "      <td>0.845561</td>\n",
       "      <td>0.843187</td>\n",
       "      <td>0.846784</td>\n",
       "      <td>0.824438</td>\n",
       "      <td>0.788354</td>\n",
       "      <td>0.758547</td>\n",
       "      <td>0.759591</td>\n",
       "      <td>0.792881</td>\n",
       "      <td>0.825145</td>\n",
       "      <td>0.850207</td>\n",
       "      <td>0.872532</td>\n",
       "      <td>0.913687</td>\n",
       "      <td>0.965626</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.939269</td>\n",
       "      <td>0.843471</td>\n",
       "      <td>0.743107</td>\n",
       "      <td>0.647496</td>\n",
       "      <td>0.565114</td>\n",
       "      <td>0.467825</td>\n",
       "      <td>0.459184</td>\n",
       "      <td>0.559254</td>\n",
       "      <td>0.657961</td>\n",
       "      <td>0.763699</td>\n",
       "      <td>0.862032</td>\n",
       "      <td>0.963696</td>\n",
       "      <td>0.993359</td>\n",
       "      <td>0.933822</td>\n",
       "      <td>0.901073</td>\n",
       "      <td>0.879900</td>\n",
       "      <td>0.836886</td>\n",
       "      <td>0.807056</td>\n",
       "      <td>0.846121</td>\n",
       "      <td>0.863678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.788836</td>\n",
       "      <td>0.801272</td>\n",
       "      <td>0.838002</td>\n",
       "      <td>0.867765</td>\n",
       "      <td>0.894126</td>\n",
       "      <td>0.921130</td>\n",
       "      <td>0.956309</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986118</td>\n",
       "      <td>0.921090</td>\n",
       "      <td>0.849009</td>\n",
       "      <td>0.773581</td>\n",
       "      <td>0.696007</td>\n",
       "      <td>0.625321</td>\n",
       "      <td>0.560554</td>\n",
       "      <td>0.485582</td>\n",
       "      <td>0.410685</td>\n",
       "      <td>0.405074</td>\n",
       "      <td>0.475198</td>\n",
       "      <td>0.548211</td>\n",
       "      <td>0.626503</td>\n",
       "      <td>0.703163</td>\n",
       "      <td>0.775538</td>\n",
       "      <td>0.850999</td>\n",
       "      <td>0.921240</td>\n",
       "      <td>0.951791</td>\n",
       "      <td>0.923895</td>\n",
       "      <td>0.905337</td>\n",
       "      <td>0.876577</td>\n",
       "      <td>0.857518</td>\n",
       "      <td>0.880043</td>\n",
       "      <td>0.883833</td>\n",
       "      <td>0.870995</td>\n",
       "      <td>0.861323</td>\n",
       "      <td>0.864892</td>\n",
       "      <td>0.863552</td>\n",
       "      <td>0.839506</td>\n",
       "      <td>0.805486</td>\n",
       "      <td>0.801828</td>\n",
       "      <td>0.826618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.917753</td>\n",
       "      <td>0.924369</td>\n",
       "      <td>0.873765</td>\n",
       "      <td>0.791381</td>\n",
       "      <td>0.699513</td>\n",
       "      <td>0.604927</td>\n",
       "      <td>0.500312</td>\n",
       "      <td>0.446012</td>\n",
       "      <td>0.528910</td>\n",
       "      <td>0.634068</td>\n",
       "      <td>0.732402</td>\n",
       "      <td>0.835828</td>\n",
       "      <td>0.920161</td>\n",
       "      <td>0.994185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.956971</td>\n",
       "      <td>0.939706</td>\n",
       "      <td>0.892600</td>\n",
       "      <td>0.901590</td>\n",
       "      <td>0.922809</td>\n",
       "      <td>0.931095</td>\n",
       "      <td>0.923393</td>\n",
       "      <td>0.915444</td>\n",
       "      <td>0.891239</td>\n",
       "      <td>0.889598</td>\n",
       "      <td>0.868729</td>\n",
       "      <td>0.824351</td>\n",
       "      <td>0.811346</td>\n",
       "      <td>0.830513</td>\n",
       "      <td>0.864741</td>\n",
       "      <td>0.875822</td>\n",
       "      <td>0.881126</td>\n",
       "      <td>0.898493</td>\n",
       "      <td>0.906304</td>\n",
       "      <td>0.857858</td>\n",
       "      <td>0.772325</td>\n",
       "      <td>0.679906</td>\n",
       "      <td>0.585949</td>\n",
       "      <td>0.492267</td>\n",
       "      <td>0.481918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.472644</td>\n",
       "      <td>0.469655</td>\n",
       "      <td>0.468433</td>\n",
       "      <td>0.482127</td>\n",
       "      <td>0.464331</td>\n",
       "      <td>0.409729</td>\n",
       "      <td>0.341709</td>\n",
       "      <td>0.278906</td>\n",
       "      <td>0.293975</td>\n",
       "      <td>0.362246</td>\n",
       "      <td>0.429817</td>\n",
       "      <td>0.494999</td>\n",
       "      <td>0.529398</td>\n",
       "      <td>0.529282</td>\n",
       "      <td>0.527002</td>\n",
       "      <td>0.544395</td>\n",
       "      <td>0.549988</td>\n",
       "      <td>0.551487</td>\n",
       "      <td>0.559017</td>\n",
       "      <td>0.558742</td>\n",
       "      <td>0.536248</td>\n",
       "      <td>0.518067</td>\n",
       "      <td>0.529951</td>\n",
       "      <td>0.561283</td>\n",
       "      <td>0.582376</td>\n",
       "      <td>0.587925</td>\n",
       "      <td>0.591390</td>\n",
       "      <td>0.574347</td>\n",
       "      <td>0.527432</td>\n",
       "      <td>0.466969</td>\n",
       "      <td>0.400774</td>\n",
       "      <td>0.380920</td>\n",
       "      <td>0.439510</td>\n",
       "      <td>0.505257</td>\n",
       "      <td>0.561538</td>\n",
       "      <td>0.577997</td>\n",
       "      <td>0.566082</td>\n",
       "      <td>0.547642</td>\n",
       "      <td>0.538735</td>\n",
       "      <td>0.527560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>0.874246</td>\n",
       "      <td>0.877014</td>\n",
       "      <td>0.864280</td>\n",
       "      <td>0.860505</td>\n",
       "      <td>0.871349</td>\n",
       "      <td>0.912404</td>\n",
       "      <td>0.958148</td>\n",
       "      <td>0.977826</td>\n",
       "      <td>0.956314</td>\n",
       "      <td>0.926773</td>\n",
       "      <td>0.906898</td>\n",
       "      <td>0.882012</td>\n",
       "      <td>0.889619</td>\n",
       "      <td>0.880813</td>\n",
       "      <td>0.831151</td>\n",
       "      <td>0.750799</td>\n",
       "      <td>0.661327</td>\n",
       "      <td>0.572037</td>\n",
       "      <td>0.480417</td>\n",
       "      <td>0.384741</td>\n",
       "      <td>0.352561</td>\n",
       "      <td>0.436437</td>\n",
       "      <td>0.526567</td>\n",
       "      <td>0.617087</td>\n",
       "      <td>0.705413</td>\n",
       "      <td>0.789439</td>\n",
       "      <td>0.845200</td>\n",
       "      <td>0.869238</td>\n",
       "      <td>0.871257</td>\n",
       "      <td>0.856014</td>\n",
       "      <td>0.851358</td>\n",
       "      <td>0.873368</td>\n",
       "      <td>0.862433</td>\n",
       "      <td>0.848354</td>\n",
       "      <td>0.850321</td>\n",
       "      <td>0.871822</td>\n",
       "      <td>0.874919</td>\n",
       "      <td>0.853888</td>\n",
       "      <td>0.862798</td>\n",
       "      <td>0.871341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.956224</td>\n",
       "      <td>0.915492</td>\n",
       "      <td>0.893433</td>\n",
       "      <td>0.874848</td>\n",
       "      <td>0.872360</td>\n",
       "      <td>0.846170</td>\n",
       "      <td>0.743772</td>\n",
       "      <td>0.630043</td>\n",
       "      <td>0.528850</td>\n",
       "      <td>0.574363</td>\n",
       "      <td>0.684415</td>\n",
       "      <td>0.784034</td>\n",
       "      <td>0.831191</td>\n",
       "      <td>0.815056</td>\n",
       "      <td>0.822067</td>\n",
       "      <td>0.833119</td>\n",
       "      <td>0.809618</td>\n",
       "      <td>0.797545</td>\n",
       "      <td>0.815231</td>\n",
       "      <td>0.842144</td>\n",
       "      <td>0.821530</td>\n",
       "      <td>0.809199</td>\n",
       "      <td>0.844743</td>\n",
       "      <td>0.889473</td>\n",
       "      <td>0.911590</td>\n",
       "      <td>0.913915</td>\n",
       "      <td>0.928354</td>\n",
       "      <td>0.918018</td>\n",
       "      <td>0.882841</td>\n",
       "      <td>0.865596</td>\n",
       "      <td>0.785103</td>\n",
       "      <td>0.676795</td>\n",
       "      <td>0.579054</td>\n",
       "      <td>0.476613</td>\n",
       "      <td>0.458748</td>\n",
       "      <td>0.565470</td>\n",
       "      <td>0.681896</td>\n",
       "      <td>0.792646</td>\n",
       "      <td>0.871660</td>\n",
       "      <td>0.872789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>0.829815</td>\n",
       "      <td>0.832084</td>\n",
       "      <td>0.852396</td>\n",
       "      <td>0.909665</td>\n",
       "      <td>0.988242</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923323</td>\n",
       "      <td>0.821865</td>\n",
       "      <td>0.721302</td>\n",
       "      <td>0.612039</td>\n",
       "      <td>0.544501</td>\n",
       "      <td>0.627801</td>\n",
       "      <td>0.742470</td>\n",
       "      <td>0.839838</td>\n",
       "      <td>0.883782</td>\n",
       "      <td>0.857296</td>\n",
       "      <td>0.836065</td>\n",
       "      <td>0.811509</td>\n",
       "      <td>0.790581</td>\n",
       "      <td>0.804179</td>\n",
       "      <td>0.828418</td>\n",
       "      <td>0.820784</td>\n",
       "      <td>0.821660</td>\n",
       "      <td>0.818403</td>\n",
       "      <td>0.817945</td>\n",
       "      <td>0.820711</td>\n",
       "      <td>0.808908</td>\n",
       "      <td>0.792135</td>\n",
       "      <td>0.788338</td>\n",
       "      <td>0.788320</td>\n",
       "      <td>0.784782</td>\n",
       "      <td>0.773148</td>\n",
       "      <td>0.783096</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>0.775193</td>\n",
       "      <td>0.775352</td>\n",
       "      <td>0.788648</td>\n",
       "      <td>0.808087</td>\n",
       "      <td>0.829057</td>\n",
       "      <td>0.841954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.886046</td>\n",
       "      <td>0.873893</td>\n",
       "      <td>0.867941</td>\n",
       "      <td>0.877090</td>\n",
       "      <td>0.884758</td>\n",
       "      <td>0.876515</td>\n",
       "      <td>0.864512</td>\n",
       "      <td>0.869479</td>\n",
       "      <td>0.872364</td>\n",
       "      <td>0.887606</td>\n",
       "      <td>0.909075</td>\n",
       "      <td>0.906887</td>\n",
       "      <td>0.891656</td>\n",
       "      <td>0.882097</td>\n",
       "      <td>0.874331</td>\n",
       "      <td>0.859243</td>\n",
       "      <td>0.843719</td>\n",
       "      <td>0.836967</td>\n",
       "      <td>0.863975</td>\n",
       "      <td>0.901079</td>\n",
       "      <td>0.946561</td>\n",
       "      <td>0.984971</td>\n",
       "      <td>0.984028</td>\n",
       "      <td>0.949422</td>\n",
       "      <td>0.912675</td>\n",
       "      <td>0.877008</td>\n",
       "      <td>0.853983</td>\n",
       "      <td>0.859501</td>\n",
       "      <td>0.871799</td>\n",
       "      <td>0.881691</td>\n",
       "      <td>0.913404</td>\n",
       "      <td>0.968015</td>\n",
       "      <td>0.992614</td>\n",
       "      <td>0.945789</td>\n",
       "      <td>0.876660</td>\n",
       "      <td>0.808906</td>\n",
       "      <td>0.741645</td>\n",
       "      <td>0.736615</td>\n",
       "      <td>0.797729</td>\n",
       "      <td>0.855637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>0.469048</td>\n",
       "      <td>0.417983</td>\n",
       "      <td>0.362322</td>\n",
       "      <td>0.351995</td>\n",
       "      <td>0.391493</td>\n",
       "      <td>0.418305</td>\n",
       "      <td>0.440135</td>\n",
       "      <td>0.444598</td>\n",
       "      <td>0.460402</td>\n",
       "      <td>0.506810</td>\n",
       "      <td>0.582590</td>\n",
       "      <td>0.671095</td>\n",
       "      <td>0.763154</td>\n",
       "      <td>0.852376</td>\n",
       "      <td>0.912775</td>\n",
       "      <td>0.863850</td>\n",
       "      <td>0.770843</td>\n",
       "      <td>0.665164</td>\n",
       "      <td>0.570234</td>\n",
       "      <td>0.466931</td>\n",
       "      <td>0.367258</td>\n",
       "      <td>0.316216</td>\n",
       "      <td>0.385087</td>\n",
       "      <td>0.454449</td>\n",
       "      <td>0.475295</td>\n",
       "      <td>0.459735</td>\n",
       "      <td>0.423755</td>\n",
       "      <td>0.393976</td>\n",
       "      <td>0.428202</td>\n",
       "      <td>0.455103</td>\n",
       "      <td>0.482325</td>\n",
       "      <td>0.508784</td>\n",
       "      <td>0.513152</td>\n",
       "      <td>0.512621</td>\n",
       "      <td>0.498838</td>\n",
       "      <td>0.479978</td>\n",
       "      <td>0.468484</td>\n",
       "      <td>0.471154</td>\n",
       "      <td>0.478454</td>\n",
       "      <td>0.478561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.770956</td>\n",
       "      <td>0.768007</td>\n",
       "      <td>0.767452</td>\n",
       "      <td>0.757746</td>\n",
       "      <td>0.742900</td>\n",
       "      <td>0.720311</td>\n",
       "      <td>0.682413</td>\n",
       "      <td>0.647154</td>\n",
       "      <td>0.659783</td>\n",
       "      <td>0.690443</td>\n",
       "      <td>0.710191</td>\n",
       "      <td>0.726423</td>\n",
       "      <td>0.739670</td>\n",
       "      <td>0.755869</td>\n",
       "      <td>0.799418</td>\n",
       "      <td>0.866029</td>\n",
       "      <td>0.900208</td>\n",
       "      <td>0.854414</td>\n",
       "      <td>0.772012</td>\n",
       "      <td>0.692737</td>\n",
       "      <td>0.607456</td>\n",
       "      <td>0.519297</td>\n",
       "      <td>0.459385</td>\n",
       "      <td>0.509832</td>\n",
       "      <td>0.591530</td>\n",
       "      <td>0.673935</td>\n",
       "      <td>0.750389</td>\n",
       "      <td>0.796189</td>\n",
       "      <td>0.777527</td>\n",
       "      <td>0.757557</td>\n",
       "      <td>0.730354</td>\n",
       "      <td>0.697465</td>\n",
       "      <td>0.714527</td>\n",
       "      <td>0.745605</td>\n",
       "      <td>0.754952</td>\n",
       "      <td>0.755059</td>\n",
       "      <td>0.755059</td>\n",
       "      <td>0.755093</td>\n",
       "      <td>0.759093</td>\n",
       "      <td>0.767555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>0.682510</td>\n",
       "      <td>0.682286</td>\n",
       "      <td>0.641051</td>\n",
       "      <td>0.620212</td>\n",
       "      <td>0.608210</td>\n",
       "      <td>0.576331</td>\n",
       "      <td>0.603596</td>\n",
       "      <td>0.645714</td>\n",
       "      <td>0.677964</td>\n",
       "      <td>0.720297</td>\n",
       "      <td>0.727853</td>\n",
       "      <td>0.782657</td>\n",
       "      <td>0.754083</td>\n",
       "      <td>0.772042</td>\n",
       "      <td>0.767737</td>\n",
       "      <td>0.772877</td>\n",
       "      <td>0.912476</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.904549</td>\n",
       "      <td>0.733804</td>\n",
       "      <td>0.566345</td>\n",
       "      <td>0.408461</td>\n",
       "      <td>0.494627</td>\n",
       "      <td>0.660626</td>\n",
       "      <td>0.822726</td>\n",
       "      <td>0.871358</td>\n",
       "      <td>0.856305</td>\n",
       "      <td>0.852545</td>\n",
       "      <td>0.805860</td>\n",
       "      <td>0.776517</td>\n",
       "      <td>0.741973</td>\n",
       "      <td>0.713029</td>\n",
       "      <td>0.763240</td>\n",
       "      <td>0.753007</td>\n",
       "      <td>0.752465</td>\n",
       "      <td>0.791204</td>\n",
       "      <td>0.761565</td>\n",
       "      <td>0.753277</td>\n",
       "      <td>0.714949</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.486236</td>\n",
       "      <td>0.561845</td>\n",
       "      <td>0.640716</td>\n",
       "      <td>0.713709</td>\n",
       "      <td>0.793515</td>\n",
       "      <td>0.869474</td>\n",
       "      <td>0.941709</td>\n",
       "      <td>0.981261</td>\n",
       "      <td>0.944207</td>\n",
       "      <td>0.920888</td>\n",
       "      <td>0.924573</td>\n",
       "      <td>0.912627</td>\n",
       "      <td>0.892986</td>\n",
       "      <td>0.901261</td>\n",
       "      <td>0.917562</td>\n",
       "      <td>0.925745</td>\n",
       "      <td>0.915685</td>\n",
       "      <td>0.911961</td>\n",
       "      <td>0.903686</td>\n",
       "      <td>0.896693</td>\n",
       "      <td>0.890374</td>\n",
       "      <td>0.878340</td>\n",
       "      <td>0.874921</td>\n",
       "      <td>0.866630</td>\n",
       "      <td>0.856727</td>\n",
       "      <td>0.867312</td>\n",
       "      <td>0.885358</td>\n",
       "      <td>0.899917</td>\n",
       "      <td>0.894387</td>\n",
       "      <td>0.888391</td>\n",
       "      <td>0.860939</td>\n",
       "      <td>0.824976</td>\n",
       "      <td>0.783459</td>\n",
       "      <td>0.761391</td>\n",
       "      <td>0.741917</td>\n",
       "      <td>0.770631</td>\n",
       "      <td>0.802701</td>\n",
       "      <td>0.821503</td>\n",
       "      <td>0.846300</td>\n",
       "      <td>0.858795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>0.792175</td>\n",
       "      <td>0.815695</td>\n",
       "      <td>0.819518</td>\n",
       "      <td>0.820559</td>\n",
       "      <td>0.847985</td>\n",
       "      <td>0.880933</td>\n",
       "      <td>0.902061</td>\n",
       "      <td>0.878266</td>\n",
       "      <td>0.838806</td>\n",
       "      <td>0.811795</td>\n",
       "      <td>0.790403</td>\n",
       "      <td>0.789433</td>\n",
       "      <td>0.790516</td>\n",
       "      <td>0.812252</td>\n",
       "      <td>0.837745</td>\n",
       "      <td>0.871234</td>\n",
       "      <td>0.835983</td>\n",
       "      <td>0.719133</td>\n",
       "      <td>0.591305</td>\n",
       "      <td>0.537446</td>\n",
       "      <td>0.646644</td>\n",
       "      <td>0.727863</td>\n",
       "      <td>0.789831</td>\n",
       "      <td>0.818977</td>\n",
       "      <td>0.811148</td>\n",
       "      <td>0.780012</td>\n",
       "      <td>0.714901</td>\n",
       "      <td>0.706178</td>\n",
       "      <td>0.775511</td>\n",
       "      <td>0.817944</td>\n",
       "      <td>0.822660</td>\n",
       "      <td>0.852053</td>\n",
       "      <td>0.858502</td>\n",
       "      <td>0.841148</td>\n",
       "      <td>0.820427</td>\n",
       "      <td>0.819915</td>\n",
       "      <td>0.837144</td>\n",
       "      <td>0.848829</td>\n",
       "      <td>0.829762</td>\n",
       "      <td>0.820849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513271</td>\n",
       "      <td>0.560516</td>\n",
       "      <td>0.602969</td>\n",
       "      <td>0.618804</td>\n",
       "      <td>0.602975</td>\n",
       "      <td>0.577985</td>\n",
       "      <td>0.551057</td>\n",
       "      <td>0.548707</td>\n",
       "      <td>0.548732</td>\n",
       "      <td>0.556318</td>\n",
       "      <td>0.600067</td>\n",
       "      <td>0.683407</td>\n",
       "      <td>0.787810</td>\n",
       "      <td>0.894806</td>\n",
       "      <td>0.974310</td>\n",
       "      <td>0.937187</td>\n",
       "      <td>0.814079</td>\n",
       "      <td>0.688203</td>\n",
       "      <td>0.563682</td>\n",
       "      <td>0.434792</td>\n",
       "      <td>0.347730</td>\n",
       "      <td>0.411929</td>\n",
       "      <td>0.518502</td>\n",
       "      <td>0.547718</td>\n",
       "      <td>0.526026</td>\n",
       "      <td>0.463647</td>\n",
       "      <td>0.378978</td>\n",
       "      <td>0.366810</td>\n",
       "      <td>0.423969</td>\n",
       "      <td>0.475823</td>\n",
       "      <td>0.516381</td>\n",
       "      <td>0.521133</td>\n",
       "      <td>0.521142</td>\n",
       "      <td>0.521143</td>\n",
       "      <td>0.522801</td>\n",
       "      <td>0.543166</td>\n",
       "      <td>0.549073</td>\n",
       "      <td>0.564977</td>\n",
       "      <td>0.576139</td>\n",
       "      <td>0.576267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows × 3060 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2     ...      3058      3059      3060\n",
       "0    0.728449  0.680755  0.619010  ...  0.805689  0.751761  0.702102\n",
       "1    0.957972  0.950695  0.941024  ...  0.910320  0.905436  0.876942\n",
       "2    0.611084  0.661575  0.695790  ...  0.361189  0.451946  0.543373\n",
       "3    0.839213  0.861690  0.866457  ...  0.805486  0.801828  0.826618\n",
       "4    0.917753  0.924369  0.873765  ...  0.547642  0.538735  0.527560\n",
       "..        ...       ...       ...  ...       ...       ...       ...\n",
       "923  0.874246  0.877014  0.864280  ...  0.792646  0.871660  0.872789\n",
       "924  0.829815  0.832084  0.852396  ...  0.736615  0.797729  0.855637\n",
       "925  0.469048  0.417983  0.362322  ...  0.755093  0.759093  0.767555\n",
       "926  0.682510  0.682286  0.641051  ...  0.821503  0.846300  0.858795\n",
       "927  0.792175  0.815695  0.819518  ...  0.564977  0.576139  0.576267\n",
       "\n",
       "[928 rows x 3060 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets try combining all 12 leads in a single csv\n",
    "location= '/content/drive/MyDrive/CMPE255_PROJECT/'\n",
    "for files in natsorted(os.listdir(location)):\n",
    "  if files.endswith(\".csv\") and not files.endswith(\"13.csv\"):\n",
    "    if files!='Combined_IDLead_1.csv':\n",
    "      df=pd.read_csv('/content/drive/MyDrive/CMPE255_PROJECT/{}'.format(files))\n",
    "      df.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "      test_final=pd.concat([test_final,df],axis=1,ignore_index=True)\n",
    "      test_final.drop(columns=test_final.columns[-1],axis=1,inplace=True)\n",
    "\n",
    "#drop the target column\n",
    "test_final.drop(columns=[255],axis=1,inplace=True)\n",
    "test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "OZ6_Lg0180y3"
   },
   "outputs": [],
   "source": [
    "#write the final file to csv\n",
    "test_final.to_csv('final_1D.csv',header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxWK-X-qjde2"
   },
   "source": [
    "#### **TEST DIMENSIONALITY REDUCTION EXPLAINED VARIANCE ON  THE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WXvZGdh5cxrL",
    "outputId": "63b7ea81-03df-43ae-b708-630b9ce6722f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of each component: [8.04649534e-02 4.68818003e-02 3.76212504e-02 2.94708618e-02\n",
      " 2.57031130e-02 2.32574514e-02 2.14376788e-02 2.04315151e-02\n",
      " 1.94482863e-02 1.79877408e-02 1.64766264e-02 1.53241665e-02\n",
      " 1.50689862e-02 1.41398267e-02 1.36330466e-02 1.33375324e-02\n",
      " 1.26355566e-02 1.25577001e-02 1.16968257e-02 1.11671338e-02\n",
      " 1.07975552e-02 1.06183806e-02 1.03402122e-02 1.01248410e-02\n",
      " 9.73197948e-03 9.25504395e-03 9.16367637e-03 8.76267060e-03\n",
      " 8.54270112e-03 8.20665462e-03 8.07642149e-03 7.90742343e-03\n",
      " 7.54929819e-03 7.21938018e-03 7.07604659e-03 6.89135251e-03\n",
      " 6.80575532e-03 6.71875790e-03 6.38252148e-03 6.33951897e-03\n",
      " 6.10254734e-03 5.94560955e-03 5.76371295e-03 5.71788829e-03\n",
      " 5.55354810e-03 5.42316932e-03 5.35640711e-03 5.08429353e-03\n",
      " 5.03302777e-03 4.96811576e-03 4.87696491e-03 4.63686128e-03\n",
      " 4.55349933e-03 4.45390625e-03 4.31579996e-03 4.28316592e-03\n",
      " 4.17213140e-03 4.12346241e-03 4.09072049e-03 3.99349122e-03\n",
      " 3.92129459e-03 3.81982060e-03 3.78116652e-03 3.73307150e-03\n",
      " 3.68894307e-03 3.55238746e-03 3.49148625e-03 3.40490507e-03\n",
      " 3.33593814e-03 3.25467389e-03 3.20023474e-03 3.14871964e-03\n",
      " 3.09091665e-03 3.07180393e-03 3.05651457e-03 2.95447952e-03\n",
      " 2.90507083e-03 2.84618700e-03 2.80939396e-03 2.76324718e-03\n",
      " 2.71487874e-03 2.68959207e-03 2.67378836e-03 2.62085254e-03\n",
      " 2.55991613e-03 2.53614502e-03 2.47015404e-03 2.45768102e-03\n",
      " 2.41851536e-03 2.39477316e-03 2.35560704e-03 2.29236345e-03\n",
      " 2.26928539e-03 2.24965527e-03 2.22764534e-03 2.19258829e-03\n",
      " 2.14654982e-03 2.09081474e-03 2.08656961e-03 2.04315332e-03\n",
      " 2.01191187e-03 1.99715030e-03 1.98092986e-03 1.93183566e-03\n",
      " 1.90133601e-03 1.86628808e-03 1.85847904e-03 1.79040117e-03\n",
      " 1.77318190e-03 1.76278440e-03 1.73682193e-03 1.70177712e-03\n",
      " 1.69142157e-03 1.66289246e-03 1.64192361e-03 1.62455779e-03\n",
      " 1.59836820e-03 1.57166872e-03 1.56017874e-03 1.55193712e-03\n",
      " 1.52130395e-03 1.50860404e-03 1.48563216e-03 1.45667689e-03\n",
      " 1.44862677e-03 1.43014707e-03 1.42443426e-03 1.39341888e-03\n",
      " 1.38941740e-03 1.38032166e-03 1.35292505e-03 1.33403513e-03\n",
      " 1.33300728e-03 1.31774024e-03 1.29238722e-03 1.24574072e-03\n",
      " 1.23408862e-03 1.21598644e-03 1.20568485e-03 1.19391143e-03\n",
      " 1.18690274e-03 1.16630751e-03 1.16159095e-03 1.14539199e-03\n",
      " 1.13634359e-03 1.11858663e-03 1.10460060e-03 1.08515359e-03\n",
      " 1.07679695e-03 1.06488284e-03 1.05861426e-03 1.04012565e-03\n",
      " 1.03222232e-03 1.02519590e-03 1.01169941e-03 9.96444257e-04\n",
      " 9.76134514e-04 9.61104386e-04 9.57134099e-04 9.48294848e-04\n",
      " 9.35386446e-04 9.29858628e-04 9.24107282e-04 9.20229599e-04\n",
      " 9.00136970e-04 8.84392791e-04 8.60041244e-04 8.58222437e-04\n",
      " 8.39586154e-04 8.34156616e-04 8.24745137e-04 8.19630377e-04\n",
      " 8.11755902e-04 8.09589697e-04 7.93351930e-04 7.83229226e-04\n",
      " 7.69323633e-04 7.62916710e-04 7.61217310e-04 7.49412461e-04\n",
      " 7.41978508e-04 7.32319449e-04 7.28386324e-04 7.15766463e-04\n",
      " 7.00416470e-04 6.92792928e-04 6.87860571e-04 6.77118996e-04\n",
      " 6.69195650e-04 6.62776506e-04 6.52787237e-04 6.41350808e-04\n",
      " 6.31671343e-04 6.25941688e-04 6.20986818e-04 6.12964320e-04\n",
      " 6.06757241e-04 6.00414979e-04 5.90442751e-04 5.85447566e-04\n",
      " 5.82053388e-04 5.72736727e-04 5.64768427e-04 5.62060875e-04\n",
      " 5.53942338e-04 5.47413376e-04 5.43815848e-04 5.39018247e-04\n",
      " 5.31538796e-04 5.21422265e-04 5.16620308e-04 5.13730678e-04\n",
      " 5.08883050e-04 5.04308686e-04 4.96238364e-04 4.91958416e-04\n",
      " 4.80055673e-04 4.74422583e-04 4.69414331e-04 4.65649131e-04\n",
      " 4.62052065e-04 4.58664175e-04 4.49131977e-04 4.46512859e-04\n",
      " 4.45747677e-04 4.36928354e-04 4.30056931e-04 4.24233413e-04\n",
      " 4.21656145e-04 4.20467965e-04 4.16760267e-04 4.15888840e-04\n",
      " 4.07286800e-04 4.03273134e-04 3.97207457e-04 3.91816959e-04\n",
      " 3.87932386e-04 3.83350761e-04 3.82143416e-04 3.79404870e-04\n",
      " 3.72582511e-04 3.63610224e-04 3.59553996e-04 3.56530336e-04\n",
      " 3.53235193e-04 3.50500778e-04 3.47735153e-04 3.42040355e-04\n",
      " 3.38310300e-04 3.37306494e-04 3.35511668e-04 3.28894993e-04\n",
      " 3.28218994e-04 3.24306980e-04 3.20401235e-04 3.14253002e-04\n",
      " 3.10208395e-04 3.06218843e-04 3.04335193e-04 3.01021082e-04\n",
      " 2.97566445e-04 2.95304829e-04 2.91254955e-04 2.89125040e-04\n",
      " 2.82693327e-04 2.76521036e-04 2.75269025e-04 2.74379696e-04\n",
      " 2.68266972e-04 2.68061648e-04 2.66099927e-04 2.64421261e-04\n",
      " 2.63834052e-04 2.57431348e-04 2.54138631e-04 2.51605639e-04\n",
      " 2.46905271e-04 2.43635722e-04 2.42082957e-04 2.40633829e-04\n",
      " 2.37784494e-04 2.37571627e-04 2.35490305e-04 2.31228801e-04\n",
      " 2.29666817e-04 2.22983440e-04 2.22297163e-04 2.19366993e-04\n",
      " 2.18709687e-04 2.16024304e-04 2.12842289e-04 2.10474739e-04\n",
      " 2.08846963e-04 2.05998344e-04 2.04966779e-04 2.02934820e-04\n",
      " 1.99807610e-04 1.96639208e-04 1.94996894e-04 1.93136310e-04\n",
      " 1.91195919e-04 1.89711177e-04 1.88308931e-04 1.84369318e-04\n",
      " 1.82566578e-04 1.80379274e-04 1.76609531e-04 1.75644704e-04\n",
      " 1.72428822e-04 1.71789330e-04 1.71183590e-04 1.69725268e-04\n",
      " 1.67457552e-04 1.65840817e-04 1.63726623e-04 1.59522818e-04\n",
      " 1.59160999e-04 1.57751649e-04 1.56405942e-04 1.54529511e-04\n",
      " 1.51823024e-04 1.51101654e-04 1.49353986e-04 1.45440930e-04\n",
      " 1.42823565e-04 1.41663369e-04 1.38458084e-04 1.37010341e-04\n",
      " 1.36519118e-04 1.35604074e-04 1.33885751e-04 1.32408220e-04\n",
      " 1.31172816e-04 1.30736357e-04 1.29593249e-04 1.25909937e-04\n",
      " 1.25109136e-04 1.23622680e-04 1.22218842e-04 1.21530832e-04\n",
      " 1.20753482e-04 1.18346392e-04 1.17315284e-04 1.15545565e-04\n",
      " 1.14204626e-04 1.13094104e-04 1.11424710e-04 1.10091416e-04\n",
      " 1.08685883e-04 1.08176805e-04 1.07089861e-04 1.04914395e-04\n",
      " 1.04118709e-04 1.02699215e-04 1.01885811e-04 1.00938278e-04\n",
      " 9.90735417e-05 9.80321595e-05 9.72807235e-05 9.51934647e-05\n",
      " 9.47306075e-05 9.41065852e-05 9.26506282e-05 9.07208806e-05\n",
      " 8.91421930e-05 8.78075879e-05 8.67414258e-05 8.58159532e-05\n",
      " 8.46912638e-05 8.38152670e-05 8.27105668e-05 8.16195287e-05\n",
      " 8.05168482e-05 8.01097405e-05 7.96248510e-05 7.84742029e-05\n",
      " 7.74759051e-05 7.68468954e-05 7.56711647e-05 7.52225433e-05\n",
      " 7.37442622e-05 7.31571140e-05 7.15227812e-05 7.11381556e-05\n",
      " 6.97528042e-05 6.92380243e-05 6.83982669e-05 6.79859787e-05\n",
      " 6.63677731e-05 6.46909994e-05 6.35827821e-05 6.30218309e-05\n",
      " 6.22874152e-05 6.13744928e-05 6.06222866e-05 6.02674245e-05\n",
      " 5.85437349e-05 5.77735788e-05 5.70138027e-05 5.61843337e-05]\n",
      "\n",
      " Total Variance Explained: 99.7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>361</th>\n",
       "      <th>362</th>\n",
       "      <th>363</th>\n",
       "      <th>364</th>\n",
       "      <th>365</th>\n",
       "      <th>366</th>\n",
       "      <th>367</th>\n",
       "      <th>368</th>\n",
       "      <th>369</th>\n",
       "      <th>370</th>\n",
       "      <th>371</th>\n",
       "      <th>372</th>\n",
       "      <th>373</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>384</th>\n",
       "      <th>385</th>\n",
       "      <th>386</th>\n",
       "      <th>387</th>\n",
       "      <th>388</th>\n",
       "      <th>389</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.457260</td>\n",
       "      <td>0.433402</td>\n",
       "      <td>-0.951313</td>\n",
       "      <td>-0.118767</td>\n",
       "      <td>0.372034</td>\n",
       "      <td>1.917341</td>\n",
       "      <td>0.503248</td>\n",
       "      <td>-0.996278</td>\n",
       "      <td>-2.306744</td>\n",
       "      <td>-1.294890</td>\n",
       "      <td>-1.029884</td>\n",
       "      <td>-3.299731</td>\n",
       "      <td>-2.371333</td>\n",
       "      <td>-1.635685</td>\n",
       "      <td>-0.600756</td>\n",
       "      <td>0.259134</td>\n",
       "      <td>0.392211</td>\n",
       "      <td>-0.432152</td>\n",
       "      <td>0.806167</td>\n",
       "      <td>-2.245053</td>\n",
       "      <td>-0.936126</td>\n",
       "      <td>0.067663</td>\n",
       "      <td>-1.856255</td>\n",
       "      <td>0.499225</td>\n",
       "      <td>0.150131</td>\n",
       "      <td>-1.026837</td>\n",
       "      <td>-0.184096</td>\n",
       "      <td>1.070505</td>\n",
       "      <td>-0.208371</td>\n",
       "      <td>-1.036157</td>\n",
       "      <td>0.917308</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>-0.379882</td>\n",
       "      <td>-2.738867</td>\n",
       "      <td>-0.150684</td>\n",
       "      <td>0.599770</td>\n",
       "      <td>0.355143</td>\n",
       "      <td>1.094111</td>\n",
       "      <td>-2.048985</td>\n",
       "      <td>-0.485500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004864</td>\n",
       "      <td>-0.070816</td>\n",
       "      <td>-0.161781</td>\n",
       "      <td>-0.052175</td>\n",
       "      <td>-0.031129</td>\n",
       "      <td>-0.053441</td>\n",
       "      <td>-0.035635</td>\n",
       "      <td>0.153516</td>\n",
       "      <td>-0.134739</td>\n",
       "      <td>0.099619</td>\n",
       "      <td>0.131589</td>\n",
       "      <td>-0.082917</td>\n",
       "      <td>-0.197682</td>\n",
       "      <td>-0.038855</td>\n",
       "      <td>-0.195225</td>\n",
       "      <td>0.161045</td>\n",
       "      <td>0.021022</td>\n",
       "      <td>-0.090870</td>\n",
       "      <td>-0.023101</td>\n",
       "      <td>-0.165461</td>\n",
       "      <td>-0.025316</td>\n",
       "      <td>0.038318</td>\n",
       "      <td>-0.106364</td>\n",
       "      <td>-0.154006</td>\n",
       "      <td>0.021192</td>\n",
       "      <td>-0.131488</td>\n",
       "      <td>0.142525</td>\n",
       "      <td>0.051453</td>\n",
       "      <td>-0.018591</td>\n",
       "      <td>-0.004467</td>\n",
       "      <td>0.049193</td>\n",
       "      <td>0.103482</td>\n",
       "      <td>0.063186</td>\n",
       "      <td>0.088417</td>\n",
       "      <td>0.006383</td>\n",
       "      <td>-0.031051</td>\n",
       "      <td>-0.019877</td>\n",
       "      <td>-0.098934</td>\n",
       "      <td>-0.141388</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.482871</td>\n",
       "      <td>-3.923833</td>\n",
       "      <td>1.001672</td>\n",
       "      <td>1.151786</td>\n",
       "      <td>-0.105634</td>\n",
       "      <td>0.935446</td>\n",
       "      <td>0.061538</td>\n",
       "      <td>1.340459</td>\n",
       "      <td>-4.633610</td>\n",
       "      <td>-0.483502</td>\n",
       "      <td>-1.864759</td>\n",
       "      <td>-0.261640</td>\n",
       "      <td>0.316998</td>\n",
       "      <td>1.534867</td>\n",
       "      <td>-0.119670</td>\n",
       "      <td>-1.397651</td>\n",
       "      <td>4.349664</td>\n",
       "      <td>-1.687776</td>\n",
       "      <td>2.018024</td>\n",
       "      <td>-0.311957</td>\n",
       "      <td>2.426070</td>\n",
       "      <td>0.611089</td>\n",
       "      <td>-1.777597</td>\n",
       "      <td>-0.349125</td>\n",
       "      <td>0.346889</td>\n",
       "      <td>0.226577</td>\n",
       "      <td>-0.776129</td>\n",
       "      <td>0.323027</td>\n",
       "      <td>-0.160008</td>\n",
       "      <td>1.492211</td>\n",
       "      <td>0.153926</td>\n",
       "      <td>0.159457</td>\n",
       "      <td>-1.210119</td>\n",
       "      <td>-0.086179</td>\n",
       "      <td>-0.769436</td>\n",
       "      <td>1.267969</td>\n",
       "      <td>-0.564659</td>\n",
       "      <td>-0.341418</td>\n",
       "      <td>-1.985230</td>\n",
       "      <td>-1.558439</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053734</td>\n",
       "      <td>0.103177</td>\n",
       "      <td>-0.188913</td>\n",
       "      <td>-0.249093</td>\n",
       "      <td>0.098583</td>\n",
       "      <td>0.006058</td>\n",
       "      <td>-0.074279</td>\n",
       "      <td>-0.063215</td>\n",
       "      <td>0.043863</td>\n",
       "      <td>-0.062570</td>\n",
       "      <td>0.136219</td>\n",
       "      <td>-0.114581</td>\n",
       "      <td>0.148691</td>\n",
       "      <td>0.103807</td>\n",
       "      <td>-0.023358</td>\n",
       "      <td>-0.124450</td>\n",
       "      <td>-0.396799</td>\n",
       "      <td>0.030457</td>\n",
       "      <td>0.144521</td>\n",
       "      <td>0.067239</td>\n",
       "      <td>-0.313200</td>\n",
       "      <td>-0.295610</td>\n",
       "      <td>0.022464</td>\n",
       "      <td>0.003461</td>\n",
       "      <td>-0.167260</td>\n",
       "      <td>-0.094348</td>\n",
       "      <td>0.059300</td>\n",
       "      <td>-0.152751</td>\n",
       "      <td>-0.139971</td>\n",
       "      <td>0.215188</td>\n",
       "      <td>-0.172374</td>\n",
       "      <td>0.095130</td>\n",
       "      <td>-0.044179</td>\n",
       "      <td>0.056281</td>\n",
       "      <td>-0.111162</td>\n",
       "      <td>-0.021412</td>\n",
       "      <td>-0.087850</td>\n",
       "      <td>-0.027603</td>\n",
       "      <td>0.038080</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.524849</td>\n",
       "      <td>-0.064380</td>\n",
       "      <td>-2.177830</td>\n",
       "      <td>-0.744632</td>\n",
       "      <td>-3.160053</td>\n",
       "      <td>-2.079289</td>\n",
       "      <td>1.991373</td>\n",
       "      <td>2.629458</td>\n",
       "      <td>1.251710</td>\n",
       "      <td>1.656291</td>\n",
       "      <td>-1.721600</td>\n",
       "      <td>1.191401</td>\n",
       "      <td>-0.059358</td>\n",
       "      <td>0.016666</td>\n",
       "      <td>-0.589977</td>\n",
       "      <td>0.277381</td>\n",
       "      <td>-1.164150</td>\n",
       "      <td>0.219596</td>\n",
       "      <td>-1.011841</td>\n",
       "      <td>2.089701</td>\n",
       "      <td>-1.209511</td>\n",
       "      <td>-0.253165</td>\n",
       "      <td>0.068591</td>\n",
       "      <td>-0.041458</td>\n",
       "      <td>1.792283</td>\n",
       "      <td>-0.735310</td>\n",
       "      <td>-0.267826</td>\n",
       "      <td>0.495506</td>\n",
       "      <td>0.728721</td>\n",
       "      <td>-1.337833</td>\n",
       "      <td>-1.338947</td>\n",
       "      <td>-1.756841</td>\n",
       "      <td>-0.396808</td>\n",
       "      <td>1.402993</td>\n",
       "      <td>0.742616</td>\n",
       "      <td>2.066993</td>\n",
       "      <td>1.737300</td>\n",
       "      <td>1.638768</td>\n",
       "      <td>0.507565</td>\n",
       "      <td>0.703938</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.215272</td>\n",
       "      <td>-0.091248</td>\n",
       "      <td>-0.138750</td>\n",
       "      <td>-0.310845</td>\n",
       "      <td>-0.114982</td>\n",
       "      <td>0.145602</td>\n",
       "      <td>0.029057</td>\n",
       "      <td>0.133017</td>\n",
       "      <td>0.018631</td>\n",
       "      <td>-0.193751</td>\n",
       "      <td>0.016406</td>\n",
       "      <td>-0.041319</td>\n",
       "      <td>0.096585</td>\n",
       "      <td>-0.167015</td>\n",
       "      <td>0.202288</td>\n",
       "      <td>0.127255</td>\n",
       "      <td>-0.040277</td>\n",
       "      <td>-0.004114</td>\n",
       "      <td>0.041241</td>\n",
       "      <td>-0.009264</td>\n",
       "      <td>-0.070678</td>\n",
       "      <td>0.100684</td>\n",
       "      <td>0.042677</td>\n",
       "      <td>-0.011628</td>\n",
       "      <td>-0.067114</td>\n",
       "      <td>0.074009</td>\n",
       "      <td>0.022301</td>\n",
       "      <td>-0.006442</td>\n",
       "      <td>0.071618</td>\n",
       "      <td>-0.080249</td>\n",
       "      <td>-0.012182</td>\n",
       "      <td>0.016389</td>\n",
       "      <td>-0.083541</td>\n",
       "      <td>0.034246</td>\n",
       "      <td>-0.002165</td>\n",
       "      <td>0.022199</td>\n",
       "      <td>-0.041591</td>\n",
       "      <td>-0.097381</td>\n",
       "      <td>-0.070855</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.626337</td>\n",
       "      <td>-0.901579</td>\n",
       "      <td>-1.351431</td>\n",
       "      <td>-1.767117</td>\n",
       "      <td>0.870166</td>\n",
       "      <td>2.210025</td>\n",
       "      <td>3.710694</td>\n",
       "      <td>-1.478115</td>\n",
       "      <td>-2.406238</td>\n",
       "      <td>-3.754295</td>\n",
       "      <td>-0.557777</td>\n",
       "      <td>-0.785967</td>\n",
       "      <td>0.520949</td>\n",
       "      <td>-3.207937</td>\n",
       "      <td>0.836712</td>\n",
       "      <td>-0.633093</td>\n",
       "      <td>2.050584</td>\n",
       "      <td>-0.264177</td>\n",
       "      <td>-0.161628</td>\n",
       "      <td>-0.922219</td>\n",
       "      <td>-1.375607</td>\n",
       "      <td>-1.401751</td>\n",
       "      <td>-3.131625</td>\n",
       "      <td>1.001584</td>\n",
       "      <td>0.895211</td>\n",
       "      <td>-2.513785</td>\n",
       "      <td>-0.518160</td>\n",
       "      <td>-0.482920</td>\n",
       "      <td>-0.045858</td>\n",
       "      <td>-0.706037</td>\n",
       "      <td>-0.248923</td>\n",
       "      <td>0.173317</td>\n",
       "      <td>-1.048835</td>\n",
       "      <td>0.131862</td>\n",
       "      <td>0.197064</td>\n",
       "      <td>0.281935</td>\n",
       "      <td>-0.382301</td>\n",
       "      <td>-0.761295</td>\n",
       "      <td>0.300483</td>\n",
       "      <td>0.957167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108555</td>\n",
       "      <td>0.285074</td>\n",
       "      <td>-0.074780</td>\n",
       "      <td>-0.244925</td>\n",
       "      <td>0.252944</td>\n",
       "      <td>0.155910</td>\n",
       "      <td>0.134332</td>\n",
       "      <td>-0.035310</td>\n",
       "      <td>-0.083564</td>\n",
       "      <td>0.087323</td>\n",
       "      <td>-0.058963</td>\n",
       "      <td>0.169122</td>\n",
       "      <td>-0.058995</td>\n",
       "      <td>0.199294</td>\n",
       "      <td>0.108662</td>\n",
       "      <td>-0.025462</td>\n",
       "      <td>0.069696</td>\n",
       "      <td>-0.229703</td>\n",
       "      <td>-0.002245</td>\n",
       "      <td>-0.042847</td>\n",
       "      <td>-0.069750</td>\n",
       "      <td>-0.220639</td>\n",
       "      <td>-0.041440</td>\n",
       "      <td>0.050481</td>\n",
       "      <td>0.104345</td>\n",
       "      <td>-0.039829</td>\n",
       "      <td>-0.097662</td>\n",
       "      <td>0.078529</td>\n",
       "      <td>-0.052380</td>\n",
       "      <td>0.048149</td>\n",
       "      <td>0.028606</td>\n",
       "      <td>-0.046752</td>\n",
       "      <td>-0.098831</td>\n",
       "      <td>0.043573</td>\n",
       "      <td>-0.119218</td>\n",
       "      <td>0.010639</td>\n",
       "      <td>-0.110523</td>\n",
       "      <td>-0.047173</td>\n",
       "      <td>-0.077733</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.863027</td>\n",
       "      <td>-2.805259</td>\n",
       "      <td>-0.428457</td>\n",
       "      <td>-0.482452</td>\n",
       "      <td>0.231498</td>\n",
       "      <td>1.977056</td>\n",
       "      <td>1.605510</td>\n",
       "      <td>-0.603866</td>\n",
       "      <td>-1.312170</td>\n",
       "      <td>3.553396</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>-0.635330</td>\n",
       "      <td>3.934087</td>\n",
       "      <td>1.739471</td>\n",
       "      <td>0.092498</td>\n",
       "      <td>-0.375677</td>\n",
       "      <td>-2.062197</td>\n",
       "      <td>-0.744017</td>\n",
       "      <td>-2.109333</td>\n",
       "      <td>-1.586144</td>\n",
       "      <td>0.190978</td>\n",
       "      <td>0.295259</td>\n",
       "      <td>0.257749</td>\n",
       "      <td>3.029742</td>\n",
       "      <td>-0.651587</td>\n",
       "      <td>-2.486830</td>\n",
       "      <td>-0.710095</td>\n",
       "      <td>-1.364978</td>\n",
       "      <td>2.452239</td>\n",
       "      <td>0.967987</td>\n",
       "      <td>-3.228894</td>\n",
       "      <td>0.355109</td>\n",
       "      <td>-1.978444</td>\n",
       "      <td>-0.721650</td>\n",
       "      <td>0.763842</td>\n",
       "      <td>0.407882</td>\n",
       "      <td>0.398858</td>\n",
       "      <td>0.598949</td>\n",
       "      <td>-1.079875</td>\n",
       "      <td>-0.552937</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059020</td>\n",
       "      <td>0.019094</td>\n",
       "      <td>0.229171</td>\n",
       "      <td>0.031877</td>\n",
       "      <td>0.046004</td>\n",
       "      <td>0.088553</td>\n",
       "      <td>-0.006622</td>\n",
       "      <td>-0.082355</td>\n",
       "      <td>-0.019582</td>\n",
       "      <td>0.157799</td>\n",
       "      <td>0.061590</td>\n",
       "      <td>-0.085825</td>\n",
       "      <td>0.208393</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>-0.107207</td>\n",
       "      <td>-0.071855</td>\n",
       "      <td>-0.078112</td>\n",
       "      <td>0.031986</td>\n",
       "      <td>-0.017471</td>\n",
       "      <td>0.030140</td>\n",
       "      <td>-0.104950</td>\n",
       "      <td>0.136329</td>\n",
       "      <td>-0.025889</td>\n",
       "      <td>-0.005956</td>\n",
       "      <td>-0.007189</td>\n",
       "      <td>0.010320</td>\n",
       "      <td>-0.010182</td>\n",
       "      <td>-0.025392</td>\n",
       "      <td>0.137030</td>\n",
       "      <td>-0.029644</td>\n",
       "      <td>-0.047303</td>\n",
       "      <td>-0.156143</td>\n",
       "      <td>0.184450</td>\n",
       "      <td>-0.053742</td>\n",
       "      <td>-0.143405</td>\n",
       "      <td>0.180017</td>\n",
       "      <td>-0.053451</td>\n",
       "      <td>-0.048106</td>\n",
       "      <td>-0.172048</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>-3.365774</td>\n",
       "      <td>-2.530997</td>\n",
       "      <td>-0.781861</td>\n",
       "      <td>0.567793</td>\n",
       "      <td>1.960806</td>\n",
       "      <td>2.319130</td>\n",
       "      <td>3.632739</td>\n",
       "      <td>2.072028</td>\n",
       "      <td>0.318600</td>\n",
       "      <td>1.423024</td>\n",
       "      <td>0.046769</td>\n",
       "      <td>-2.914288</td>\n",
       "      <td>2.490308</td>\n",
       "      <td>-1.509974</td>\n",
       "      <td>-0.024407</td>\n",
       "      <td>0.645754</td>\n",
       "      <td>-0.431200</td>\n",
       "      <td>-0.915645</td>\n",
       "      <td>-2.197339</td>\n",
       "      <td>-0.921664</td>\n",
       "      <td>1.041247</td>\n",
       "      <td>-2.904255</td>\n",
       "      <td>2.019000</td>\n",
       "      <td>-1.680969</td>\n",
       "      <td>-1.166711</td>\n",
       "      <td>1.819737</td>\n",
       "      <td>2.469585</td>\n",
       "      <td>0.986770</td>\n",
       "      <td>-1.946829</td>\n",
       "      <td>-0.108685</td>\n",
       "      <td>-1.217637</td>\n",
       "      <td>-0.532643</td>\n",
       "      <td>1.279253</td>\n",
       "      <td>1.959304</td>\n",
       "      <td>-0.985623</td>\n",
       "      <td>1.175776</td>\n",
       "      <td>1.855441</td>\n",
       "      <td>0.439515</td>\n",
       "      <td>1.439233</td>\n",
       "      <td>0.986529</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026936</td>\n",
       "      <td>-0.033183</td>\n",
       "      <td>0.121917</td>\n",
       "      <td>-0.067305</td>\n",
       "      <td>-0.209145</td>\n",
       "      <td>-0.093162</td>\n",
       "      <td>-0.185985</td>\n",
       "      <td>0.137584</td>\n",
       "      <td>0.067961</td>\n",
       "      <td>-0.099507</td>\n",
       "      <td>0.059101</td>\n",
       "      <td>0.108274</td>\n",
       "      <td>0.045860</td>\n",
       "      <td>-0.018358</td>\n",
       "      <td>-0.143598</td>\n",
       "      <td>0.011703</td>\n",
       "      <td>0.134274</td>\n",
       "      <td>-0.051352</td>\n",
       "      <td>0.023441</td>\n",
       "      <td>-0.164377</td>\n",
       "      <td>-0.168790</td>\n",
       "      <td>0.049493</td>\n",
       "      <td>-0.131794</td>\n",
       "      <td>0.017603</td>\n",
       "      <td>0.132835</td>\n",
       "      <td>-0.000645</td>\n",
       "      <td>0.007095</td>\n",
       "      <td>0.087813</td>\n",
       "      <td>0.054866</td>\n",
       "      <td>0.021954</td>\n",
       "      <td>-0.031794</td>\n",
       "      <td>0.023750</td>\n",
       "      <td>-0.079201</td>\n",
       "      <td>-0.046390</td>\n",
       "      <td>0.010010</td>\n",
       "      <td>-0.296469</td>\n",
       "      <td>0.072683</td>\n",
       "      <td>-0.001818</td>\n",
       "      <td>-0.040645</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>-6.340754</td>\n",
       "      <td>0.634164</td>\n",
       "      <td>-3.259888</td>\n",
       "      <td>2.894988</td>\n",
       "      <td>2.625952</td>\n",
       "      <td>-1.435685</td>\n",
       "      <td>1.570297</td>\n",
       "      <td>0.456509</td>\n",
       "      <td>-0.855480</td>\n",
       "      <td>0.523852</td>\n",
       "      <td>-2.268330</td>\n",
       "      <td>1.391630</td>\n",
       "      <td>0.059511</td>\n",
       "      <td>-0.724329</td>\n",
       "      <td>-0.173057</td>\n",
       "      <td>0.532177</td>\n",
       "      <td>-1.341757</td>\n",
       "      <td>-1.101967</td>\n",
       "      <td>-2.343420</td>\n",
       "      <td>-2.531835</td>\n",
       "      <td>-1.032105</td>\n",
       "      <td>-0.578936</td>\n",
       "      <td>1.095615</td>\n",
       "      <td>-0.885507</td>\n",
       "      <td>-2.002129</td>\n",
       "      <td>-0.810205</td>\n",
       "      <td>0.440234</td>\n",
       "      <td>-3.439951</td>\n",
       "      <td>-1.466192</td>\n",
       "      <td>-1.834589</td>\n",
       "      <td>-0.326115</td>\n",
       "      <td>1.845900</td>\n",
       "      <td>1.362090</td>\n",
       "      <td>-1.728924</td>\n",
       "      <td>-1.786918</td>\n",
       "      <td>0.074898</td>\n",
       "      <td>0.534308</td>\n",
       "      <td>-1.953397</td>\n",
       "      <td>0.025380</td>\n",
       "      <td>-1.984023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010375</td>\n",
       "      <td>0.159853</td>\n",
       "      <td>-0.100748</td>\n",
       "      <td>-0.109698</td>\n",
       "      <td>-0.099151</td>\n",
       "      <td>0.070785</td>\n",
       "      <td>0.160624</td>\n",
       "      <td>-0.291796</td>\n",
       "      <td>0.084703</td>\n",
       "      <td>0.129967</td>\n",
       "      <td>0.048631</td>\n",
       "      <td>-0.029079</td>\n",
       "      <td>-0.109343</td>\n",
       "      <td>0.066468</td>\n",
       "      <td>-0.044219</td>\n",
       "      <td>-0.062293</td>\n",
       "      <td>-0.130258</td>\n",
       "      <td>0.033404</td>\n",
       "      <td>0.015828</td>\n",
       "      <td>0.107923</td>\n",
       "      <td>-0.013249</td>\n",
       "      <td>-0.000710</td>\n",
       "      <td>-0.078391</td>\n",
       "      <td>0.149710</td>\n",
       "      <td>-0.011521</td>\n",
       "      <td>0.106566</td>\n",
       "      <td>-0.094433</td>\n",
       "      <td>-0.190267</td>\n",
       "      <td>-0.240952</td>\n",
       "      <td>-0.007091</td>\n",
       "      <td>-0.254852</td>\n",
       "      <td>0.204865</td>\n",
       "      <td>0.061479</td>\n",
       "      <td>-0.062016</td>\n",
       "      <td>-0.017977</td>\n",
       "      <td>-0.179619</td>\n",
       "      <td>-0.128965</td>\n",
       "      <td>0.114953</td>\n",
       "      <td>0.002204</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>-2.106097</td>\n",
       "      <td>4.489429</td>\n",
       "      <td>4.740686</td>\n",
       "      <td>2.521037</td>\n",
       "      <td>1.810682</td>\n",
       "      <td>1.252919</td>\n",
       "      <td>-0.925497</td>\n",
       "      <td>2.413092</td>\n",
       "      <td>-0.426080</td>\n",
       "      <td>-2.335654</td>\n",
       "      <td>-1.531288</td>\n",
       "      <td>-0.620642</td>\n",
       "      <td>1.282804</td>\n",
       "      <td>0.045448</td>\n",
       "      <td>2.148036</td>\n",
       "      <td>-0.471078</td>\n",
       "      <td>-1.721987</td>\n",
       "      <td>1.046936</td>\n",
       "      <td>-0.119067</td>\n",
       "      <td>1.229584</td>\n",
       "      <td>-0.913725</td>\n",
       "      <td>0.044197</td>\n",
       "      <td>0.982729</td>\n",
       "      <td>1.584395</td>\n",
       "      <td>0.451917</td>\n",
       "      <td>-1.185538</td>\n",
       "      <td>0.853646</td>\n",
       "      <td>0.860246</td>\n",
       "      <td>1.036336</td>\n",
       "      <td>0.698754</td>\n",
       "      <td>1.047132</td>\n",
       "      <td>0.008582</td>\n",
       "      <td>-0.527476</td>\n",
       "      <td>0.228246</td>\n",
       "      <td>0.845918</td>\n",
       "      <td>-0.406428</td>\n",
       "      <td>0.217244</td>\n",
       "      <td>0.711842</td>\n",
       "      <td>0.221686</td>\n",
       "      <td>0.612309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102118</td>\n",
       "      <td>-0.037680</td>\n",
       "      <td>-0.111782</td>\n",
       "      <td>-0.109949</td>\n",
       "      <td>0.268776</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>-0.217273</td>\n",
       "      <td>0.057457</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>-0.053787</td>\n",
       "      <td>-0.022633</td>\n",
       "      <td>0.055987</td>\n",
       "      <td>-0.101333</td>\n",
       "      <td>-0.145087</td>\n",
       "      <td>-0.015617</td>\n",
       "      <td>-0.023460</td>\n",
       "      <td>0.042270</td>\n",
       "      <td>0.027257</td>\n",
       "      <td>-0.078770</td>\n",
       "      <td>-0.171411</td>\n",
       "      <td>0.106483</td>\n",
       "      <td>-0.028574</td>\n",
       "      <td>0.071645</td>\n",
       "      <td>0.321761</td>\n",
       "      <td>0.076134</td>\n",
       "      <td>0.034416</td>\n",
       "      <td>0.183309</td>\n",
       "      <td>-0.065949</td>\n",
       "      <td>-0.141772</td>\n",
       "      <td>-0.072015</td>\n",
       "      <td>-0.019943</td>\n",
       "      <td>-0.048166</td>\n",
       "      <td>0.024671</td>\n",
       "      <td>-0.065024</td>\n",
       "      <td>-0.112313</td>\n",
       "      <td>0.171747</td>\n",
       "      <td>-0.074065</td>\n",
       "      <td>0.057723</td>\n",
       "      <td>0.220157</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>-1.091978</td>\n",
       "      <td>7.063249</td>\n",
       "      <td>2.692234</td>\n",
       "      <td>-0.543705</td>\n",
       "      <td>-0.443376</td>\n",
       "      <td>1.028903</td>\n",
       "      <td>-1.749000</td>\n",
       "      <td>0.469497</td>\n",
       "      <td>-0.575796</td>\n",
       "      <td>-1.181286</td>\n",
       "      <td>1.873092</td>\n",
       "      <td>-0.101693</td>\n",
       "      <td>-3.235010</td>\n",
       "      <td>-0.185095</td>\n",
       "      <td>2.613143</td>\n",
       "      <td>-0.394172</td>\n",
       "      <td>-1.826231</td>\n",
       "      <td>0.521815</td>\n",
       "      <td>1.463937</td>\n",
       "      <td>-1.732318</td>\n",
       "      <td>0.908233</td>\n",
       "      <td>1.024110</td>\n",
       "      <td>1.823084</td>\n",
       "      <td>-3.377902</td>\n",
       "      <td>-1.318873</td>\n",
       "      <td>0.091852</td>\n",
       "      <td>-0.156378</td>\n",
       "      <td>-1.067764</td>\n",
       "      <td>2.349830</td>\n",
       "      <td>2.596215</td>\n",
       "      <td>2.269263</td>\n",
       "      <td>-0.140556</td>\n",
       "      <td>-0.401830</td>\n",
       "      <td>-1.618476</td>\n",
       "      <td>0.681230</td>\n",
       "      <td>2.091503</td>\n",
       "      <td>1.024831</td>\n",
       "      <td>-0.655318</td>\n",
       "      <td>1.568348</td>\n",
       "      <td>0.291136</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023274</td>\n",
       "      <td>-0.052643</td>\n",
       "      <td>-0.166015</td>\n",
       "      <td>0.122907</td>\n",
       "      <td>-0.092177</td>\n",
       "      <td>0.184605</td>\n",
       "      <td>-0.068945</td>\n",
       "      <td>0.242647</td>\n",
       "      <td>-0.016888</td>\n",
       "      <td>0.004289</td>\n",
       "      <td>-0.129357</td>\n",
       "      <td>0.233832</td>\n",
       "      <td>-0.196152</td>\n",
       "      <td>0.071088</td>\n",
       "      <td>0.027378</td>\n",
       "      <td>0.047544</td>\n",
       "      <td>-0.032370</td>\n",
       "      <td>-0.074678</td>\n",
       "      <td>0.084778</td>\n",
       "      <td>-0.058096</td>\n",
       "      <td>-0.233877</td>\n",
       "      <td>-0.000594</td>\n",
       "      <td>-0.044000</td>\n",
       "      <td>0.117960</td>\n",
       "      <td>-0.035313</td>\n",
       "      <td>-0.092788</td>\n",
       "      <td>0.012120</td>\n",
       "      <td>0.017404</td>\n",
       "      <td>0.127023</td>\n",
       "      <td>-0.033270</td>\n",
       "      <td>-0.134814</td>\n",
       "      <td>0.087036</td>\n",
       "      <td>-0.106924</td>\n",
       "      <td>-0.048681</td>\n",
       "      <td>0.135341</td>\n",
       "      <td>-0.150403</td>\n",
       "      <td>0.038602</td>\n",
       "      <td>-0.022517</td>\n",
       "      <td>-0.122067</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>6.346374</td>\n",
       "      <td>-3.865858</td>\n",
       "      <td>6.209416</td>\n",
       "      <td>1.247192</td>\n",
       "      <td>0.112905</td>\n",
       "      <td>0.278156</td>\n",
       "      <td>-0.507665</td>\n",
       "      <td>-0.342197</td>\n",
       "      <td>-0.152731</td>\n",
       "      <td>-3.172417</td>\n",
       "      <td>-0.933201</td>\n",
       "      <td>-0.904980</td>\n",
       "      <td>-0.217569</td>\n",
       "      <td>-0.258681</td>\n",
       "      <td>0.488851</td>\n",
       "      <td>-1.142714</td>\n",
       "      <td>0.442364</td>\n",
       "      <td>-0.687637</td>\n",
       "      <td>0.466094</td>\n",
       "      <td>-0.755363</td>\n",
       "      <td>-0.453395</td>\n",
       "      <td>1.036853</td>\n",
       "      <td>-0.753808</td>\n",
       "      <td>0.159117</td>\n",
       "      <td>-0.376229</td>\n",
       "      <td>0.688768</td>\n",
       "      <td>0.168990</td>\n",
       "      <td>-2.205062</td>\n",
       "      <td>0.356973</td>\n",
       "      <td>0.116352</td>\n",
       "      <td>0.090819</td>\n",
       "      <td>-0.365147</td>\n",
       "      <td>0.492074</td>\n",
       "      <td>-0.281468</td>\n",
       "      <td>-0.341162</td>\n",
       "      <td>-0.765518</td>\n",
       "      <td>-0.559919</td>\n",
       "      <td>0.811706</td>\n",
       "      <td>0.132620</td>\n",
       "      <td>1.195814</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.135423</td>\n",
       "      <td>0.153313</td>\n",
       "      <td>0.007477</td>\n",
       "      <td>-0.050110</td>\n",
       "      <td>0.019978</td>\n",
       "      <td>-0.153267</td>\n",
       "      <td>-0.035308</td>\n",
       "      <td>-0.017003</td>\n",
       "      <td>0.066704</td>\n",
       "      <td>-0.220810</td>\n",
       "      <td>0.074226</td>\n",
       "      <td>-0.000821</td>\n",
       "      <td>-0.098753</td>\n",
       "      <td>-0.080564</td>\n",
       "      <td>0.071457</td>\n",
       "      <td>0.254252</td>\n",
       "      <td>0.012325</td>\n",
       "      <td>0.074889</td>\n",
       "      <td>0.165686</td>\n",
       "      <td>-0.041542</td>\n",
       "      <td>-0.183783</td>\n",
       "      <td>0.223062</td>\n",
       "      <td>0.030537</td>\n",
       "      <td>-0.113039</td>\n",
       "      <td>0.072626</td>\n",
       "      <td>0.109512</td>\n",
       "      <td>0.165781</td>\n",
       "      <td>0.098440</td>\n",
       "      <td>0.006251</td>\n",
       "      <td>0.035250</td>\n",
       "      <td>0.067124</td>\n",
       "      <td>0.151706</td>\n",
       "      <td>-0.074048</td>\n",
       "      <td>-0.087132</td>\n",
       "      <td>0.140888</td>\n",
       "      <td>-0.044756</td>\n",
       "      <td>0.026722</td>\n",
       "      <td>0.106166</td>\n",
       "      <td>0.070446</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows × 401 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2  ...       398       399  target\n",
       "0   -0.457260  0.433402 -0.951313  ... -0.098934 -0.141388       2\n",
       "1   -1.482871 -3.923833  1.001672  ... -0.027603  0.038080       2\n",
       "2    6.524849 -0.064380 -2.177830  ... -0.097381 -0.070855       2\n",
       "3   -4.626337 -0.901579 -1.351431  ... -0.047173 -0.077733       2\n",
       "4   -3.863027 -2.805259 -0.428457  ... -0.048106 -0.172048       2\n",
       "..        ...       ...       ...  ...       ...       ...     ...\n",
       "923 -3.365774 -2.530997 -0.781861  ... -0.001818 -0.040645       3\n",
       "924 -6.340754  0.634164 -3.259888  ...  0.114953  0.002204       3\n",
       "925 -2.106097  4.489429  4.740686  ...  0.057723  0.220157       3\n",
       "926 -1.091978  7.063249  2.692234  ... -0.022517 -0.122067       3\n",
       "927  6.346374 -3.865858  6.209416  ...  0.106166  0.070446       3\n",
       "\n",
       "[928 rows x 401 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now Perform Dimensionality reduction (PCA) on that Dataframe and check\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#do PCA and choose componeents as 400\n",
    "pca = PCA(n_components=400)\n",
    "x_pca = pca.fit_transform(test_final)\n",
    "x_pca = pd.DataFrame(x_pca)\n",
    "\n",
    "# Calculate the variance explained by priciple components\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print('Variance of each component:', pca.explained_variance_ratio_)\n",
    "print('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))\n",
    "\n",
    "#store the new pca generated dimensions in a dataframe\n",
    "#store the new pca generated dimensions in a dataframe\n",
    "pca_df = pd.DataFrame(data = x_pca)\n",
    "target = pd.Series(result_df.iloc[:,-1], name='target')\n",
    "final_result_df = pd.concat([pca_df, target], axis=1)\n",
    "final_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Rfdk0yXlO6rz"
   },
   "outputs": [],
   "source": [
    "#save to dimensionally reduced csv file\n",
    "final_result_df.to_csv(\"pca_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n18ljrh0KJB3",
    "outputId": "7a6f991c-c1e9-430e-c97c-a4d8c2542876"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PCA_ECG.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "#save the PCA model\n",
    "joblib_file='PCA_ECG.pkl'\n",
    "joblib.dump(pca,joblib_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l93cv3DXjw2A"
   },
   "source": [
    "#### **TRYING DIFFERENT ML MODELS ON THE ALL 12 LEADS COMBINED FILE WITHOUT DIMENSIONALITY REDUCTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLAdjoV0j5oS"
   },
   "source": [
    "##### **KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Np5A0l30TYEI",
    "outputId": "9e5f4a0e-90d6-4853-a687-f4e69a70de1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.793010752688172\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.65      0.76       105\n",
      "           1       0.95      0.91      0.93        94\n",
      "           2       0.70      0.86      0.77       112\n",
      "           3       0.65      0.74      0.69        61\n",
      "\n",
      "    accuracy                           0.79       372\n",
      "   macro avg       0.80      0.79      0.79       372\n",
      "weighted avg       0.81      0.79      0.79       372\n",
      "\n",
      "Tuned Model Parameters: {'knn__n_neighbors': 1}\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline steps:\n",
    "steps = [('knn', KNeighborsClassifier())]\n",
    "        \n",
    "# Create the pipeline: pipeline \n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# have paased less range value of hyperparamter since i'm using free tier version of google colab.\n",
    "k_range = list(range(1, 30))\n",
    "parameters = dict(knn__n_neighbors=k_range)\n",
    "\n",
    "#input\n",
    "X = final_result_df.iloc[:,:-1]\n",
    "\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "# Create train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "\n",
    "#increasing cv score takes lot of time in gooogle colab, so kept it just 2.\n",
    "cv = GridSearchCV(pipeline,parameters,cv=2)\n",
    "\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "Knn_Accuracy = cv.score(X_test, y_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(Knn_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "METf9MJdkESh"
   },
   "source": [
    "##### **LOGISTIC REGRESSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "3qBayEUskDfc"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline steps:\n",
    "steps = [('lr', LogisticRegression())]\n",
    "        \n",
    "# Create the pipeline: pipeline \n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "#input\n",
    "X = final_result_df.iloc[:,:-1]\n",
    "\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "#parameters for gridsearchcv\n",
    "c_space = np.logspace(-4, 4, 10)\n",
    "parameters = {'lr__C': c_space,'lr__penalty': ['l2']} \n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "\n",
    "#call GridSearchCV and set crossvalscore to 2\n",
    "cv = GridSearchCV(pipeline,parameters,cv=2)\n",
    "\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = cv.predict(X_test)\n",
    "LR_Accuracy = cv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_xV9YlHZktj9",
    "outputId": "b6fc9558-a064-407b-ced4-c81e3aefbcee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7768817204301075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.57      0.68       105\n",
      "           1       0.83      0.91      0.87        94\n",
      "           2       0.82      0.86      0.84       112\n",
      "           3       0.59      0.77      0.67        61\n",
      "\n",
      "    accuracy                           0.78       372\n",
      "   macro avg       0.77      0.78      0.76       372\n",
      "weighted avg       0.79      0.78      0.77       372\n",
      "\n",
      "Tuned Model Parameters: {'lr__C': 0.3593813663804626, 'lr__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(LR_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOJDLUA6p5Xd"
   },
   "source": [
    "##### **SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yG0rz8crlJK5",
    "outputId": "a560d998-8fd8-4fe9-bac6-be22cda925b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9051724137931034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.92      0.86       119\n",
      "           1       1.00      1.00      1.00       125\n",
      "           2       0.91      0.89      0.90       140\n",
      "           3       0.93      0.78      0.84        80\n",
      "\n",
      "    accuracy                           0.91       464\n",
      "   macro avg       0.91      0.89      0.90       464\n",
      "weighted avg       0.91      0.91      0.91       464\n",
      "\n",
      "Tuned Model Parameters: {'SVM__C': 10, 'SVM__gamma': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline\n",
    "steps = [('SVM', SVC())]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "#input\n",
    "X = final_result_df.iloc[:,:-1]\n",
    "\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "# Specify the hyperparameter space, if we increase the penalty(c) and gamma value the accurancy can be increased.\n",
    "#since it takes lots of time in google colab provided only a single value\n",
    "parameters = {'SVM__C':[1, 10, 100],\n",
    "              'SVM__gamma':[0.1, 0.01]}\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5,random_state=21)\n",
    "\n",
    "cv = GridSearchCV(pipeline,parameters,cv=3)\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "SVM_Accuracy = cv.score(X_test, y_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "SVM_Accuracy=cv.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy: {}\".format(SVM_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqxwP-4vIbbm"
   },
   "source": [
    "### **XGBOOST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4xWCZUjyIWoJ"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "bzdU_OL7IY0U"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3OY33WUdIapb",
    "outputId": "40e112ea-0807-47bc-a873-0bdeeca4f0c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.853448275862069\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.70      0.74       119\n",
      "           1       0.98      1.00      0.99       125\n",
      "           2       0.82      0.87      0.84       140\n",
      "           3       0.80      0.82      0.81        80\n",
      "\n",
      "    accuracy                           0.85       464\n",
      "   macro avg       0.85      0.85      0.85       464\n",
      "weighted avg       0.85      0.85      0.85       464\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: {}\".format(accuracy))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izUvTTDBp-Vy"
   },
   "source": [
    "#### **SAVING A VERY BASIC ML MODEL AND USING IT ON REALTIME PIPELINE TO CHECK WORKING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mN7RIL_3PnJb",
    "outputId": "6bcad7b1-cfbc-4066-f8c2-5404e4e6da11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knn_model_test.pkl']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import joblib\n",
    "#input\n",
    "X = final_result_df.iloc[:,:-1]\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "# Create train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "\n",
    "knn =  KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "joblib_file='knn_model_test.pkl'\n",
    "joblib.dump(knn,joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNiBzgDv_-oA",
    "outputId": "e68d2bfc-1935-404e-de27-3628e2d9e0c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model_test.pkl']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "#input\n",
    "X = pd.read_csv('final_1D.csv',header=None)\n",
    "\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5,random_state=21)\n",
    "\n",
    "svm=SVC(C=10,gamma=0.01)\n",
    "\n",
    "svm.fit(X_train,y_train)\n",
    "\n",
    "joblib_file='svm_model_test.pkl'\n",
    "joblib.dump(svm,joblib_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmKRxfCBH7Yo"
   },
   "source": [
    "### **ENSEMBLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7V2P-WtiYZ4f"
   },
   "outputs": [],
   "source": [
    "# Importing required modules\n",
    "from sklearn import linear_model, tree, ensemble\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eA1TkdrcYZ4g"
   },
   "outputs": [],
   "source": [
    "#input\n",
    "X = final_result_df.iloc[:,0:-1]\n",
    "\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4caey39YZ4f"
   },
   "outputs": [],
   "source": [
    "# Stacking of ML Models\n",
    "eclf = VotingClassifier(estimators=[ \n",
    "    ('SVM', SVC(probability=True)),\n",
    "    ('knn', KNeighborsClassifier()),\n",
    "    ('rf', ensemble.RandomForestClassifier()),\n",
    "    ('bayes',GaussianNB()),\n",
    "    ('logistic',LogisticRegression()),\n",
    "    ], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "neqxcy3GqS91",
    "outputId": "5acb921a-9719-4836-8ef2-4b7ba370e27b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SVM__C': 1, 'SVM__gamma': 0.1, 'knn__n_neighbors': 1, 'rf__n_estimators': 300}\n",
      "Accuracy: 0.9247311827956989\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92        80\n",
      "           1       1.00      1.00      1.00        72\n",
      "           2       0.92      0.92      0.92        79\n",
      "           3       0.88      0.75      0.81        48\n",
      "\n",
      "    accuracy                           0.92       279\n",
      "   macro avg       0.92      0.91      0.91       279\n",
      "weighted avg       0.92      0.92      0.92       279\n",
      "\n",
      "{'SVM__C': 1, 'SVM__gamma': 0.1, 'knn__n_neighbors': 1, 'rf__n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning using gridSearch\n",
    "params = {'SVM__C':[1, 10, 100],\n",
    "          'SVM__gamma':[0.1, 0.01],\n",
    "          'knn__n_neighbors': [1,3,5],\n",
    "          'rf__n_estimators':[300, 400],\n",
    "          }\n",
    "\n",
    "grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n",
    "voting_clf = grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "Voting_Accuracy=voting_clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy: {}\".format(Voting_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(voting_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91lBO_ZUYZ4j"
   },
   "outputs": [],
   "source": [
    "# open a file, where you ant to store the data\n",
    "file = open('Heart_Disease_Prediction_using_ECG.pkl', 'wb')\n",
    "# dump information to that file\n",
    "pickle.dump(voting_clf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Emu__tW7qP9b"
   },
   "source": [
    "## SAVE AND USE THE ABOVE MODEL IN THE STREAMLIT APP : **https://colab.research.google.com/drive/139YVmcUBCiP52J2sX3QE_eiu2sukVgpn?usp=sharing**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Merging_Scaled_1D_&_Trying_Different_CLassification_ML_Models_.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
